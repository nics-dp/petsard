{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PETs_eval_anonymeter():\n",
    "    import PETs_Tool\n",
    "\n",
    "    def __init__(self\n",
    "                ,data_ori ,data_syn ,data_control\n",
    "                ,params={} ,**kwargs):\n",
    "        ####### ####### #######\n",
    "        # init - params       #\n",
    "        ####### ####### #######\n",
    "        # {'evaluate_params' : {'anonymeter':\n",
    "        default_params = {'SinglingOut' : {'univariate'   : 'Y'\n",
    "                                          ,'multivariate' : 'N'\n",
    "                                          }\n",
    "                         ,'SinglingOut_params' : {'univariate'   : {'n_attacks' : 500}\n",
    "                                                 ,'multivariate' : {'n_attacks' : 500\n",
    "                                                                   ,'n_cols'    : 2\n",
    "                                                                   }\n",
    "                                                 }\n",
    "                         ,'Linkability' : 'Y'\n",
    "                         ,'Linkability_params' : {'n_attacks'   : 2000\n",
    "                                                 ,'aux_cols'    : []\n",
    "                                                 # [TODO] study how to set n_neighbors\n",
    "                                                 ,'n_neighbors' : 10\n",
    "                                                 # n_jobs follow joblib convention.\n",
    "                                                 # -1 = all cores,\n",
    "                                                 # -2 = all execept one\n",
    "                                                 ,'n_jobs'      : -2\n",
    "                                                 }\n",
    "                         ,'Inference' : 'Y'\n",
    "                         ,'Inference_params' : {'n_attacks'   : 1000\n",
    "                                               ,'secret'      : ''\n",
    "                                               # n_jobs follow joblib convention.\n",
    "                                               # -1 = all cores,\n",
    "                                               # -2 = all execept one\n",
    "                                               ,'n_jobs'      : -2\n",
    "                                               }\n",
    "                         }\n",
    "        self.params = PETs_Tool.PETs_util.update_append_nested(default_params ,params)\n",
    "        self.data_ori     = data_ori\n",
    "        self.data_syn     = data_syn\n",
    "        self.data_control = data_control\n",
    "        self.evaluators   = {}\n",
    "        self.results      = {}\n",
    "\n",
    "        import warnings\n",
    "        with warnings.catch_warnings():\n",
    "            # anonymeter\\evaluators\\singling_out_evaluator.py:97:\n",
    "            # FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version.\n",
    "            # Use isinstance(dtype, CategoricalDtype) instead elif is_categorical_dtype(values)\n",
    "            warnings.simplefilter(\"ignore\" ,category=FutureWarning)\n",
    "            # c:\\Program Files\\Python310\\lib\\site-packages\\anonymeter\\stats\\confidence.py:215:\n",
    "            # UserWarning: Attack is as good or worse as baseline model.\n",
    "            # Estimated rates: attack = 0.30674239114619767, baseline = 0.30773856438771213.\n",
    "            # Analysis results cannot be trusted. self._sanity_check()\n",
    "            warnings.simplefilter(\"ignore\" ,category=UserWarning)\n",
    "\n",
    "            ####### ####### #######\n",
    "            # init - SinglingOut  #\n",
    "            ####### ####### #######\n",
    "            if any(v == 'Y' for v in self.params['SinglingOut'].values()):\n",
    "                self.evaluators['SinglingOut'] = {}\n",
    "                self.results[   'SinglingOut'] = {}\n",
    "\n",
    "                if self.params['SinglingOut']['univariate'] == 'Y':\n",
    "                    self.__SinglingOutEvaluator(mode = 'univariate'\n",
    "                                               ,**self.params['SinglingOut_params']['univariate']\n",
    "                                               )\n",
    "                # if self.params['SinglingOut']['multivariate'] == 'Y':\n",
    "                #     self.__SinglingOutEvaluator(mode = 'multivariate'\n",
    "                #                                ,**self.params['SinglingOut_params']['multivariate']\n",
    "                #                                )\n",
    "            ####### ####### #######\n",
    "            # init - SinglingOut  #\n",
    "            ####### ####### #######\n",
    "            if self.params['Linkability'] == 'Y':\n",
    "                self.__LinkabilityEvaluator(**self.params['Linkability_params'])\n",
    "\n",
    "\n",
    "            if self.params['Inference'] == 'Y':\n",
    "                self.evaluators['Inference'] = {}\n",
    "                self.results[   'Inference'] = {}\n",
    "                if self.params['Inference_params']['secret'] == '':\n",
    "                    for __secret in self.data_syn.columns:\n",
    "                        self.params['Inference_params']['secret'] = __secret\n",
    "                        self.__InferenceEvaluator(**self.params['Inference_params'])\n",
    "                else:\n",
    "                    self.__InferenceEvaluator(**self.params['Inference_params'])\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    ####### ####### #######\n",
    "    # SinglingOut         #\n",
    "    ####### ####### #######\n",
    "    def __SinglingOutEvaluator(self ,mode ,**kwargs):\n",
    "        import time\n",
    "        __time_start = time.time()\n",
    "        print('Now is Singling-Out Evaluator')\n",
    "        from anonymeter import evaluators\n",
    "        __evaluator = evaluators.SinglingOutEvaluator(ori       = self.data_ori\n",
    "                                                     ,syn       = self.data_syn\n",
    "                                                     ,control   = self.data_control\n",
    "                                                     ,**kwargs\n",
    "                                                     )\n",
    "        try:\n",
    "            __evaluator.evaluate(mode=mode)\n",
    "            self.evaluators['SinglingOut'][mode] = __evaluator\n",
    "            self.results[   'SinglingOut'][mode] = self.__Result(self.evaluators['SinglingOut'][mode])\n",
    "        except RuntimeError as ex:\n",
    "            print(f\"Singling out evaluation failed with {ex}.\"\n",
    "                   \"Please re-run this cell.\"\n",
    "                   \"For more stable results increase `n_attacks`. Note that this will \"\n",
    "                   \"make the evaluation slower.\")\n",
    "        # multivariance\n",
    "        # try:\n",
    "        #     __evaluator.evaluate(mode='multivariate')\n",
    "        #     __risk = __evaluator.risk()\n",
    "        #     print(__risk)\n",
    "        #     res = __evaluator.results()\n",
    "        #     print(__evaluator.queries()[:3])\n",
    "        # except RuntimeError as ex:\n",
    "        #     print(f\"Singling out evaluation failed with {ex}. Please re-run this cell.\"\n",
    "        #           \"For more stable results increase `n_attacks`. Note that this will \"\n",
    "        #           \"make the evaluation slower.\")\n",
    "        print(f\"Singling-Out Evaluator time: {round(time.time()-__time_start ,4)} sec.\")\n",
    "\n",
    "\n",
    "\n",
    "    ####### ####### #######\n",
    "    # Linkability         #\n",
    "    ####### ####### #######\n",
    "    def __LinkabilityEvaluator(self ,n_jobs ,aux_cols ,**kwargs):\n",
    "        import time\n",
    "        __time_start = time.time()\n",
    "        print('Now is Linkability Evaluator')\n",
    "        from anonymeter import evaluators\n",
    "        __str_aux_cols = \"\\nand \".join(f\"[{', '.join(row)}]\" for row in aux_cols)\n",
    "        print(f\"aux_cols are {__str_aux_cols}.\")\n",
    "        __evaluator = evaluators.LinkabilityEvaluator(ori      = self.data_ori\n",
    "                                                     ,syn      = self.data_syn\n",
    "                                                     ,control  = self.data_control\n",
    "                                                     ,aux_cols = aux_cols\n",
    "                                                     ,**kwargs\n",
    "                                                     )\n",
    "        __evaluator.evaluate(n_jobs=n_jobs)\n",
    "        self.evaluators['Linkability'] = __evaluator\n",
    "        self.results[   'Linkability'] = self.__Result(self.evaluators['Linkability'])\n",
    "        print(f\"Linkability Evaluator time: {round(time.time()-__time_start ,4)} sec.\")\n",
    "\n",
    "\n",
    "\n",
    "    ####### ####### #######\n",
    "    # Inference           #\n",
    "    ####### ####### #######\n",
    "    def __InferenceEvaluator(self ,secret ,n_jobs ,**kwargs):\n",
    "        import time\n",
    "        __time_start = time.time()\n",
    "        print(f\"Now is Inference Evaluator: secret is {secret}.\")\n",
    "        from anonymeter import evaluators\n",
    "        __aux_cols = [col for col in self.data_syn.columns if col != secret]\n",
    "        __evaluator = evaluators.InferenceEvaluator(ori      = self.data_ori\n",
    "                                                   ,syn      = self.data_syn\n",
    "                                                   ,control  = self.data_control\n",
    "                                                   ,aux_cols = __aux_cols\n",
    "                                                   ,secret   = secret\n",
    "                                                   ,**kwargs\n",
    "                                                   )\n",
    "        __evaluator.evaluate(n_jobs=n_jobs)\n",
    "        self.evaluators['Inference'][secret] = __evaluator\n",
    "        self.results[   'Inference'][secret] = self.__Result(self.evaluators['Inference'][secret])\n",
    "        print(f\"Inference Evaluator time: {round(time.time()-__time_start ,4)} sec.\")\n",
    "\n",
    "\n",
    "\n",
    "    ####### ####### #######\n",
    "    # Result              #\n",
    "    ####### ####### #######\n",
    "    def __Result(self ,__evaluator): \n",
    "        import numpy as np\n",
    "        __dict_result    = {}\n",
    "        __para_to_handle = [('Risk'              , ['risk()'    ,'value'                 ])\n",
    "                           ,('Risk_CI_btm'       , ['risk()'    ,'ci[0]'                 ])\n",
    "                           ,('Risk_CI_top'       , ['risk()'    ,'ci[1]'                 ])\n",
    "                           ,('Attack_Rate'       , ['results()' ,'attack_rate'   ,'value'])\n",
    "                           ,('Attack_Rate_err'   , ['results()' ,'attack_rate'   ,'error'])\n",
    "                           ,('Baseline_Rate'     , ['results()' ,'baseline_rate' ,'value'])\n",
    "                           ,('Baseline_Rate_err' , ['results()' ,'baseline_rate' ,'error'])\n",
    "                           ,('Control_Rate'      , ['results()' ,'control_rate'  ,'value'])\n",
    "                           ,('Control_Rate_err'  , ['results()' ,'control_rate'  ,'error'])\n",
    "                           ]\n",
    "        for __key ,__attrs in __para_to_handle:\n",
    "            try:\n",
    "                __attr_value = __evaluator\n",
    "                for __attr in __attrs:\n",
    "                    if '()' in __attr:\n",
    "                        __method_name = __attr.split('(')[0]\n",
    "                        if hasattr(__attr_value ,__method_name):\n",
    "                            __method = getattr(__attr_value ,__method_name)\n",
    "                            if callable(__method):\n",
    "                                __attr_value = __method()\n",
    "                            else:\n",
    "                                __dict_result[__key] = np.nan\n",
    "                                break\n",
    "                        else:\n",
    "                            __dict_result[__key] = np.nan\n",
    "                            break\n",
    "                    elif '[' in __attr:\n",
    "                        __attr_name = __attr.split('[')[0]\n",
    "                        __index = int(__attr.split('[')[1].rstrip(']'))\n",
    "                        if hasattr(__attr_value, __attr_name)\\\n",
    "                        and isinstance(getattr(__attr_value, __attr_name) ,(list, dict, tuple)):\n",
    "                            try:\n",
    "                                __attr_value = getattr(__attr_value  ,__attr_name)[__index]\n",
    "                            except (IndexError, KeyError):\n",
    "                                __dict_result[__key] = np.nan\n",
    "                                break\n",
    "                        else:\n",
    "                            __dict_result[__key] = np.nan\n",
    "                            break\n",
    "                    else:\n",
    "                        __attr_value = getattr(__attr_value, __attr)\n",
    "                __dict_result[__key] = __attr_value\n",
    "            except Exception as e:\n",
    "                __dict_result[__key] = np.nan\n",
    "        return __dict_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now is [Adult]: [Adt Income] adult.csv comparision.\n",
      "Now is [Adult] original data been load: [Adt Income] adult.csv.\n",
      "Bootstrap df: 5 times is done.\n",
      "Now is [Adult] by library SDV.\n",
      "Now is [Adult] by method GaussianCoupula in library SDV.\n",
      "Trail as [Adult]_SDV_GaussianCoupula_1.\n",
      "Metafile loading time: 0.029 sec.\n",
      "We are execute SingleTable - GaussianCoupula.\n",
      "Model training time: 20.6221 sec.\n",
      "Sample as same as raw data: # 48842 rows data by GaussianCoupula in 2.6976 秒\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[90], line 139\u001b[0m\n\u001b[0;32m    136\u001b[0m __Synther\u001b[39m.\u001b[39mdata_syn     \u001b[39m=\u001b[39m PETs_Tool\u001b[39m.\u001b[39mPETs_SD_SDV(__Synther ,__Synther_param)\u001b[39m.\u001b[39msynthetic_data\n\u001b[0;32m    137\u001b[0m __Synther\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m __data\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m--> 139\u001b[0m \u001b[39mfor\u001b[39;00m same_sample_time \u001b[39min\u001b[39;00m same_sample_times:\n\u001b[0;32m    140\u001b[0m     __trail_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m__trail_name\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(same_sample_time\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mzfill(digits_same_sample)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    141\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTrail as \u001b[39m\u001b[39m{\u001b[39;00m__trail_name\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PETs_Tool\n",
    "\n",
    "\n",
    "folder_raw = '.\\\\data'\n",
    "folder_SD  = '.\\\\data_dpsd'\n",
    "\n",
    "\n",
    "\n",
    "dict_PETs_setting = {'adult' : {'filename'    : '[Adult]'\n",
    "                               ,'read_params' : {'downcast'       : 'N' # 'Y'\n",
    "                                                ,'label_encoding' : 'Y'}\n",
    "                            #    ,'describe_params' : {'data': {'?'}}\n",
    "                               }\n",
    "                    }\n",
    "dict_filename = {'adult': {'raw' : '[Adt Income] adult.csv'\n",
    "                          }\n",
    "                }\n",
    "dict_filename = {key: {**value, 'params': dict_PETs_setting.get(key, {})} \n",
    "                 for key, value in dict_filename.items()}\n",
    "# print(dict_filename)\n",
    "\n",
    "\n",
    "\n",
    "# update default in SDV\n",
    "list_adult_SD_SDV_train = ['GaussianCoupula'\n",
    "                        #    'CoupulaGAN'\n",
    "                        #   ,'CTGAN'\n",
    "                        #   ,'GaussianCoupula'\n",
    "                        #   ,'TVAE'\n",
    "                          ]\n",
    "dict_adult_SD_SDV_train = {k: {'sd_params': {'model': k}} for k in list_adult_SD_SDV_train}\n",
    "dict_filename = {key: PETs_Tool.PETs_util.update_append_nested(value\n",
    "                                                              ,{'SD_train': dict_adult_SD_SDV_train}\n",
    "                                                              ) \n",
    "                 for key, value in dict_filename.items()}\n",
    "dict_adult_SD_SDV_train_default = {'sd_params': {'save_model': 'N'}\n",
    "                                  ,'sample'   : 'Y'\n",
    "                                  ,'sample_params' : {'sample_rows_as_raw' : 'Y'\n",
    "                                                     ,'save_data' : 'N'}\n",
    "                                  }\n",
    "dict_adult_SD_train = {'SDV' : {key: PETs_Tool.PETs_util.update_append_nested(dict_adult_SD_SDV_train_default\n",
    "                                                                       ,dict_adult_SD_SDV_train.get(key ,{})\n",
    "                                                                       )\n",
    "                                for key in dict_adult_SD_SDV_train\n",
    "                               }\n",
    "                      }\n",
    "dict_filename = {k1: {k2: dict_adult_SD_train if k2 == 'SD_train' else v2\n",
    "                      for k2 ,v2 in dict_filename[k1].items()\n",
    "                     } \n",
    "                 for k1 in dict_filename\n",
    "                }\n",
    "# print(dict_filename)\n",
    "\n",
    "\n",
    "same_sample_times = 5\n",
    "dict_bootstrap = {'bootstrap_params' : {'bootstrap_time' : 5 # 3 # 30\n",
    "                                       ,'sample_ratio'   : 0.8\n",
    "                                       ,'random_state'   : None\n",
    "                                       }\n",
    "                 ,'index_params' : {'index_save'      : 'N'\n",
    "                                   ,'index_save_data' : 'N'\n",
    "                                   ,'index_filename'  : 'Anomymeter'\n",
    "                                   }\n",
    "                 }\n",
    "# import json\n",
    "# print(json.dumps(dict_bootstrap ,indent=4))\n",
    "\n",
    "__Synther_param = {'sd_params': {'metadata' : 'SingleTable'\n",
    "                                ,'save_model' : 'N'}\n",
    "                  ,'sample' : 'Y'\n",
    "                  ,'sample_params' : {'sample_rows_as_raw' : 'Y'\n",
    "                                     ,'save_data' : 'N'}\n",
    "                  }\n",
    "aux_cols = [['age' ,'fnlwgt' ,'race' ,'gender' ,'native-country']\n",
    "           ,['workclass' ,'education' ,'capital-gain' ,'capital-loss' ,'hours-per-week']\n",
    "           ]\n",
    "\n",
    "__param_anonymeter = {'SinglingOut': {'univariate'   : 'Y'\n",
    "                                     ,'multivariate' : 'N'\n",
    "                                     }\n",
    "                     ,'Linkability' : 'Y'\n",
    "                     ,'Linkability_params' : {'aux_cols' : aux_cols}\n",
    "                     ,'Inference' : 'Y'\n",
    "                     ,'Inference_params' : {'secret' : ''} # all columns\n",
    "                     }\n",
    "def flatten_dict(d, parent_key='', sep='_'):\n",
    "    items = []\n",
    "    for key, value in d.items():\n",
    "        new_key = f\"{parent_key}{sep}{key}\" if parent_key else key\n",
    "        if isinstance(value, dict):\n",
    "            items.extend(flatten_dict(value, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, value))\n",
    "    return dict(items)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "__dict_anonymeter_result = {}\n",
    "for __dataset ,v in dict_filename.items():\n",
    "    __filename = v['params']['filename']\n",
    "    print(f\"Now is {__filename}: {v['raw']} comparision.\")\n",
    "\n",
    "    __Loader = PETs_Tool.PETs_Loader(os.path.join(folder_raw ,v['raw'])\n",
    "                                    ,v['params']\n",
    "                                    )\n",
    "    __df_ori = __Loader.data\n",
    "    print(f\"Now is {__filename} original data been load: {v['raw']}.\")\n",
    "\n",
    "    dict_boostrap_index = PETs_Tool.PETs_util.df_bootstrap(__df_ori ,dict_bootstrap)\n",
    "    digits_max_boostrap = len(str(max(dict_boostrap_index.keys()))) # max boostrap times for fill zero\n",
    "    digits_same_sample  = len(str(    same_sample_times          )) # same sample times for fill zero         \n",
    "\n",
    "    for __library ,__dict_sd in v['SD_train'].items():\n",
    "        print(f\"Now is {__filename} by library {__library}.\")\n",
    "\n",
    "        for __method ,__params in __dict_sd.items():\n",
    "            print(f\"Now is {__filename} by method {__method} in library {__library}.\")\n",
    "\n",
    "            import copy\n",
    "            for time ,dict_idx in dict_boostrap_index.items():\n",
    "                __trail_name = f\"{__filename}_{__library}_{__method}_{str(time+1).zfill(digits_max_boostrap)}\"\n",
    "                print(f\"Trail as {__trail_name}.\")\n",
    "\n",
    "                ####### ####### #######\n",
    "                # init - ori ,ctrl ,syn #\n",
    "                ####### ####### #######\n",
    "                __Synther = copy.deepcopy(__Loader)\n",
    "                __data = __Synther.data.copy()\n",
    "                __Synther.data_ori     = __data.loc[dict_boostrap_index[time]['idx_train'     ]].reset_index(drop=True)\n",
    "                __Synther.data_control = __data.loc[dict_boostrap_index[time]['idx_validation']].reset_index(drop=True)\n",
    "                __Synther.data         = __Synther.data_ori.copy()\n",
    "                __Synther.data_syn     = PETs_Tool.PETs_SD_SDV(__Synther ,__Synther_param).synthetic_data\n",
    "                __Synther.data = __data.copy()\n",
    "\n",
    "                for same_sample_time in range(same_sample_times):\n",
    "                    __trail_name = f\"{__trail_name}_{str(same_sample_time+1).zfill(digits_same_sample)}\"\n",
    "                    print(f\"Trail as {__trail_name}.\")\n",
    "                    ####### ####### #######\n",
    "                    # _anonymeter         #\n",
    "                    ####### ####### #######\n",
    "                    __anonymeter = PETs_eval_anonymeter(data_ori     = __Synther.data_ori\n",
    "                                                    ,data_syn     = __Synther.data_syn\n",
    "                                                    ,data_control = __Synther.data_control\n",
    "                                                    ,params = __param_anonymeter\n",
    "                                                    )\n",
    "                    __evaluator = __anonymeter.evaluators\n",
    "                    \n",
    "                    __dict_anonymeter_result[__trail_name] = {}\n",
    "                    for __result_key ,__result_value in __anonymeter.results.items():\n",
    "                        __dict_anonymeter_result[__trail_name].update(flatten_dict(__result_value ,parent_key=__result_key))\n",
    "\n",
    "                # 20231116, Justyn: refer to\n",
    "                # https://github.com/statice/anonymeter/blob/main/notebooks/anonymeter_example.ipynb\n",
    "                # https://storage.googleapis.com/statice-public/anonymeter-datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "__df_result = pd.DataFrame.from_dict(__dict_anonymeter_result ,orient='index')\n",
    "# [TODO] 按照 SinglingOut/Linkability/Inference 以及 colname 排序\n",
    "__df_result.to_csv('Justyn_anonymeter_20231128_1301.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 這是已經有檔案的做法 (Unfinished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_adult_SD_SDV = {'CoupulaGAN'      : ['[Adt Income]_[SDV]_[CoupulaGAN]_20231106_113038_Asia_Taipei_10000.csv']\n",
    "                    ,'CTGAN'           : ['[Adt Income]_[SDV]_[CTGAN]_20231106_093702_Asia_Taipei_10000.csv']\n",
    "                    ,'GaussianCoupula' : ['[Adt Income]_[SDV]_[GaussianCoupula]_20231106_093622_Asia_Taipei_10000.csv']\n",
    "                    ,'TVAE'            : ['[Adt Income]_[SDV]_[TVAE]_20231106_105252_Asia_Taipei_10000.csv']\n",
    "                    }\n",
    "dict_filename = {key: {**value ,'SDV': dict_adult_SD_SDV} \n",
    "                 for key, value in dict_filename.items()}\n",
    "\n",
    "\n",
    "\n",
    "for __dataset ,v in dict_filename.items():\n",
    "    __filename = v['params']['filename']\n",
    "    print(f\"Now is {__filename}: {v['raw']} comparision.\")\n",
    "\n",
    "    __df_ori = PETs_Tool.PETs_Loader(os.path.join(folder_raw ,v['raw'])\n",
    "                                    ,v['params']\n",
    "                                    ).data\n",
    "    print(f\"Now is {__filename} original data been load: {v['raw']}.\")\n",
    "\n",
    "    for __library ,__dict_sd in v['SD'].items():\n",
    "        print(f\"Now is {__filename} by library {__library}.\")\n",
    "\n",
    "        for __method ,__list_sd in __dict_sd.items():\n",
    "            print(f\"Now is {__filename} by method {__method} in library {__library}.\")\n",
    "            __list_sd = [__list_sd] if isinstance(__list_sd ,str) else __list_sd\n",
    "            for __filename_sd in __list_sd:\n",
    "                # 20231116, Justyn: 這裡是合成資料有落地\n",
    "                #                   如果必要，可以直接調用 PETs_SD_* 結果的.synthetic_data\n",
    "                __df_syn = PETs_Tool.PETs_Loader(os.path.join(folder_SD ,__filename_sd)\n",
    "                                                ,v['params']\n",
    "                                                ).data\n",
    "                print(f\"Now is {__filename} synthetic data been load: {__filename_sd}.\")\n",
    "\n",
    "                # 20231116, Justyn: refer to\n",
    "                # https://github.com/statice/anonymeter/blob/main/notebooks/anonymeter_example.ipynb\n",
    "                # https://storage.googleapis.com/statice-public/anonymeter-datasets\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
