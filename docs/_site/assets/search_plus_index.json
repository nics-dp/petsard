{"/PETsARD-Gitbook/pages/about/": {
    "title": "About",
    "keywords": "Jekyll",
    "url": "/PETsARD-Gitbook/pages/about/",
    "body": "This is an about page."
  },"/PETsARD-Gitbook/pages/contact/": {
    "title": "Contact",
    "keywords": "Jekyll",
    "url": "/PETsARD-Gitbook/pages/contact/",
    "body": "This is an contact page."
  },"/PETsARD-Gitbook/pages/design/draft/": {
    "title": "Design Draft",
    "keywords": "Jekyll",
    "url": "/PETsARD-Gitbook/pages/design/draft/",
    "body": "This is an draft page."
  },"/PETsARD-Gitbook/Benchmark-datasets.html": {
    "title": "Benchmark datasets",
    "keywords": "",
    "url": "/PETsARD-Gitbook/Benchmark-datasets.html",
    "body": "Benchmark datasets Benchmark datasets is an extended feature of the Loader module in PETsARD (Benchmarker), providing users with convenient and reliable example data for easier algorithm applicability analysit or Privacy-enhancement evaluating. Therefore, this document will focus on introducing various datasets. For details on how to use Loader, please refer to the Loader documentation. Using benchmark datasets is straightforward. You only need to place the corresponding “Benchmark name” label for each dataset in the filepath parameter of Loader in the format benchmark://{Benchmark name} (case-insensitive). PETsARD will then download the corresponding dataset and load it into Loader.data, allowing you to customize the dataset’s metadata according to other Loader parameters. Here is an example of calling the “adult” dataset: 基準資料集 (Benchmark datasets) 是 PETsARD 的 Loader 模組的延伸功能 (Benchmarker)，提供使用者方便呼叫、且可靠的範例資料，讓後續的演算法適用性分析或隱私強化驗測都更為方便。因此，本文將著重在各資料集的介紹上，關於 Loader 的使用方式詳見 Loader 文檔。 基準資料集的使用非常簡單，你只要將各資料集對應的 “Benchmark name” 標籤，以 benchmark://{Benchmark name} 的形式放到 Loader 的 filepath 參數中（大小寫不限），PETsARD 便會將對應的資料集下載好，並遵照 Loader 的功能加載在 Loader.data，而你仍可以按照 Loader 的其他參數去自定義資料集的 metadata。以下是呼叫 “adult” 資料集的例子： loader = PETsARD.Loader( filepath='benchmark://adult', na_values={k: '?' for k in [ 'workclass', 'occupation', 'native-country' ]} ) print(loader.data.head(1)) data lists Name benchmark://{Benchmark}: The labels for benchmark datasets, used as input, are case-insensitive. 基準資料集的標籤，用於輸入，大小寫不限。 Filename ( ./benchmark/{Benchmark filename}): The actual file will be stored locally and read with the filename. 實際會存到本地、並讀取資料的檔名。 Access: Public or Private. 公開或私有。 Columns: Columns number. 欄位數。 Rows: Rows number. 行數。 File size: File size. 檔案大小。 License: License of datasets. 資料集的授權。 Hash: Top 6 digits of Hash values. 哈希值的前六位。 Name Filename Access Columns Rows File size License Hash adult adult.csv Public 15 48,842 5 MB CC BY 4.0 1f13ee alarm TODO Public TODO TODO TODO TODO TODO car_insurance TODO Public TODO TODO TODO TODO TODO coil2000 TODO Public TODO TODO TODO TODO TODO covtype TODO Public TODO TODO TODO CC BY-NC-SA 4.0 TODO ds_salaries ds_salaries.csv Public 11 607 TODO CC0 TODO expedia_hotel_logs TODO Public TODO TODO TODO TODO TODO intrusion TODO Public TODO TODO TODO TODO TODO nhanes_diabetes B.csv Public 12 4,190 &lt;1 MB TODO TODO smoking_driking smoking_driking_dataset_Ver01.csv Public 24 991,346 104 MB CC BY-NC-SA 4.0 TODO uk_us_pf_household TODO Public TODO 3,094,494 167 MB TODO TODO uk_us_pf_person TODO Public TODO 7,688,060 83 MB TODO TODO uk_us_pf_prop_net TODO Public TODO 166,971,542 6,117 MB TODO TODO us_census_1940 TODO Private TODO TODO TODO Restricted TODO adult Name: Adult Alias: Adult income, Census Income Subject Area: Social Science Precision: 1 Person 1 records Columns: 15 Continuous: TODO Datetime: 0 Discrete: TODO Float: TODO String: TODO Int: TODO Missing %: Yes (TODO) Special NA value: “?” Hash: 1f13ee2bf9d7c66098429281ab91fa1b51cbabd3b805cc365b3c6b44491ea2c0 Filtered extraction from 1994 US Census. 來自 1994 年美國人口普查的過濾資料。 loader = PETsARD.Loader( filepath='benchmark://adult', na_values={k: '?' for k in [ 'workclass', 'occupation', 'native-country' ]} ) print(loader.data.head(1)) https://archive.ics.uci.edu/dataset/2/adult https://archive.ics.uci.edu/dataset/20/census+income https://www.kaggle.com/datasets/wenruliu/adult-income-dataset alarm Name: A Logical Alarm Reduction Mechanism (ALARM) monitoring system (synthetic) data set Alias: Subject Area: Health and Medicine Precision: 1 House 1 records Columns: TODO Continuous: TODO Datetime: TODO Discrete: TODO Float: TODO String: TODO Int: TODO Missing %: Yes (TODO) Special NA value: TODO https://www.bnlearn.com/documentation/man/alarm.html car_insurance Name: Insurance evaluation network (synthetic) data set Alias: insurance Subject Area: Business Precision: 1 Person 1 records Columns: TODO Continuous: TODO Datetime: TODO Discrete: TODO Float: TODO String: TODO Int: TODO Missing %: Yes (TODO) Special NA value: TODO The naming as “car_insurance” is because the name “insurance” may be confused with many datasets provided by insurance companies on Kaggle. This is just a temporary name. 命名為 “car_insurance” 是因為 “insurance” 這個名稱可能會跟許多 Kaggle 上保險公司提供的資料集混淆。這只是暫時的命名。 coil2000 Name: Insurance Company Benchmark (COIL 2000) Alias: Subject Area: Social Science Precision: 1 Person 1 records Columns: 86 Continuous: TODO Datetime: TODO Discrete: TODO Float: TODO String: TODO Int: TODO Missing %: Yes (TODO) Special NA value: TODO The caravan insurance dataset from the Netherlands, used for the Computational Intelligence and Learning (CoIL) Challenge 2000 in Europe. 荷蘭的房車保險資料集，用於歐洲計算智能和學習（CoIL）挑戰賽 2000。 https://archive.ics.uci.edu/dataset/125/insurance+company+benchmark+coil+2000 covtype Name: Forest cover types datasets Alias: Subject Area: Climate and Enviorment Precision: 1 geospatial 1 records Columns: TODO Continuous: TODO Datetime: TODO Discrete: TODO Float: TODO String: TODO Int: TODO Missing %: Yes (TODO) Special NA value: TODO This study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado, combined data from US Forest Service (USFS) and US Geological Survey (USGS). Predicting forest cover type from cartographic variables only. 這個研究區域包括科羅拉多北部羅斯福國家森林中的四個荒野地區，結合了美國森林服務（USFS）和美國地質調查（USGS）的數據。僅從地圖變數預測森林覆蓋類型。 https://archive.ics.uci.edu/dataset/31/covertype ds_salaries Name: Data Science Jobs Salaries Dataset Alias: Subject Area: Business Precision: 1 Person 1 records Columns: TODO Continuous: TODO Datetime: TODO Discrete: TODO Float: TODO String: TODO Int: TODO Missing %: Yes (TODO) Special NA value: TODO Salary data for Data Scientists from 2020 to 2021 sourced from ai-jobs.net. 來自 ai-jobs.net 的 2020~2021年中的資料科學家薪資資料。 https://www.kaggle.com/datasets/saurabhshahane/data-science-jobs-salaries https://ai-jobs.net/salaries/form/ expedia_hotel_logs Name: Expedia Hotel Recommendations datasets Alias: Subject Area: Computer Science Precision: 1 Recommendations 1 records Columns: TODO Continuous: TODO Datetime: TODO Discrete: TODO Float: TODO String: TODO Int: TODO Missing %: Yes (TODO) Special NA value: TODO From Expedia Hotel Recommendations competitions in Kaggle. 來自 Kaggle 的 Expedia 飯店推薦競賽。 https://www.kaggle.com/competitions/expedia-hotel-recommendations/data intrusion Name: Intrusion Detector Learning Alias: Subject Area: Computer Science Precision: 1 Connection 1 records Columns: TODO Continuous: TODO Datetime: TODO Discrete: TODO Float: TODO String: TODO Int: TODO Missing %: Yes (TODO) Special NA value: TODO This dataset contains network traffic with simulated attacks on a U.S. Air Force LAN and was prepared and managed by MIT Lincoln Labs for the 1998 DARPA Intrusion Detection Evaluation Program, aimed at surveying and evaluating research in intrusion detection. The dataset provides a standardized set of audited data, including a wide variety of intrusions simulated in a military network environment. A version of this dataset was used in the 1999 KDD intrusion detection contest. 這個資料集包含模擬對美國空軍局域網的攻擊的網絡流量，是為了 1998 年 DARPA 入侵檢測評估計劃而由MIT林肯實驗室準備的，旨在調查和評估入侵檢測領域的研究，並使用在 1999 年的 KDD 入侵檢測競賽。 https://kdd.ics.uci.edu/databases/kddcup99/task.html iris Name: Fisher’s Iris data set Alias: Subject Area: Biology Precision: 1 Organism 1 records Columns: TODO Continuous: TODO Datetime: TODO Discrete: TODO Float: TODO String: TODO Int: TODO Missing %: Yes (TODO) Special NA value: TODO The Iris dataset, was originally collected by Edgar Anderson to quantify morphological variations in Iris flowers and made famous by Ronald Fisher in 1936. It includes measurements of sepals and petals from three Iris species, usually used to develop a species-distinguishing model in machine learning field. Iris 資料集，最初是由 Edgar Anderson 收集以量化鳶尾花的形態變異，並在1936年被 Ronald Fisher 使用而知名。它包括三個鳶尾花物種的花萼和花瓣的測量，在機器學習領域上常用於開發區分物種的模型。 來自 Kaggle 的 Expedia 飯店推薦競賽。 https://en.wikipedia.org/wiki/Iris_flower_data_set nhanes_diabetes Name: National Health and Nutrition Examination Survey (NHANES) 2015-2016 diabetes Alias: Subject Area: Health and Medicine Precision: 1 Person 1 records Columns: 12 Continuous: TODO Datetime: TODO Discrete: TODO Float: TODO String: TODO Int: TODO Missing %: TODO (TODO) Special NA value: TODO “nhanes_diabetes” is integrated from subsets of NHANES data, and is used in PWSCup2021. Composition details can be found in PWSCup2021 GitHub activ_diabet9_csv.py. Involves the following subdatasets: “nhanes_diabetes” is named this way because “diabet” is commonly used in the community, especially on Kaggle, to refer to other datasets. “nhanes_diabetes” 是由 NHANES 資料子集整合而成，並在 PWSCup2021 中使用。有關組成詳情可在 PWSCup2021 GitHub 的 activ_diabet9_csv.py 中找到。涉及以下子資料集： DIQ_I - [Questionnaire Data] Diabetes [問卷資料] 糖尿病 BMX_I - [Examination Data] Body Measures [檢查資料] 身體測量 PAQ_I - [Questionnaire Data] Physical Activity [問卷資料] 體育活動 GHB_I - [Laboratory Data] Glycohemoglobin [實驗室資料] 醣化血色素 DPQ_I - [Questionnaire Data] Mental Health - Depression Screener [問卷資料] 心理健康 - 憂鬱篩檢 INQ_I - [Questionnaire Data] Income [問卷資料] 收入 因為 “diabet” 在社群上、尤其是 Kaggle、通常是指其他的資料集，所以本資料集命名為 “nhanes_diabetes”。 loader = PETsARD.Loader( filepath='benchmark://nhanes_diabetes', header_exist=False, header_names=[ 'gen','age','race','edu','mar', 'bmi','dep','pir','gh','mets', 'qm','dia' ] ) print(loader.data.head(1)) https://www.iwsec.org/pws/2021/index.html https://github.com/kikn88/pwscup2021 https://wwwn.cdc.gov/nchs/nhanes/continuousnhanes/default.aspx?BeginYear=2015 https://hackmd.io/@petworks/rJ-UOh9Rn/https%3A%2F%2Fhackmd.io%2F%40petworks%2Fr15yF3zYT smoking_driking Name: Smoking and Drinking Dataset with body signal Alias: Subject Area: Health and Medicine Precision: 1 Person 1 records Columns: 24 Continuous: TODO Datetime: TODO Discrete: TODO Float: TODO String: TODO Int: TODO Missing %: Yes (TODO) Special NA value: TODO Provided by the National Health Insurance Service in Korea, a dataset on smoking, drinking, and body signal (physiological indicators) with sensitive information removed. Dataset labels are named based on the original dataset file names. 由韓國健康保險公團提供，去除機敏資料的抽菸、飲酒、與生理指標資料集。資料集標籤按照原始資料集檔案名稱取名。 https://www.kaggle.com/datasets/sooyoungher/smoking-drinking-dataset/data uk_us_pf_household Name: UK-US PETs prize challenges - Pandemic Forecasting: Household datasets Alias: Subject Area: Health and Medicine Precision: 1 House 1 records Columns: TODO Continuous: TODO Datetime: TODO Discrete: TODO Float: TODO String: TODO Int: TODO Missing %: Yes (TODO) Special NA value: TODO The data is sourced from the Synthetic Epidemiology Challenge within the U.S.-UK Privacy Enhancement Technologies prize challenges. It should be noted that it currently only includes data entity two provided by the University of Virginia from the U.S., and selects only three representative datasets. 資料來源於美國英國隱私強化技術比賽當中的合成流行病學比賽項目。須注意的是目前只包含美國方的維吉尼亞大學提供的資料實體二，且僅挑選三個代表性資料集。 https://petsprizechallenges.com/ https://prepare-vo.org/synthetic-pandemic-outbreaks https://dataverse.lib.virginia.edu/dataset.xhtml?persistentId=doi:10.18130/V3/ZOG1FF https://net.science/files/resources/datasets/PET_Prize_PandemicForecasting/ uk_us_pf_person Name: UK-US PETs prize challenges - Pandemic Forecasting: Person datasets Alias: Subject Area: Health and Medicine Precision: 1 Person 1 records Columns: TODO Continuous: TODO Datetime: TODO Discrete: TODO Float: TODO String: TODO Int: TODO Missing %: Yes (TODO) Special NA value: TODO See uk_us_pf_household for more details. 詳情請見 uk_us_pf_household。 uk_us_pf_prop_net Name: UK-US PETs prize challenges - Pandemic Forecasting: Population Network datasets Alias: Subject Area: Health and Medicine Precision: 1 Connection 1 records Columns: TODO Continuous: TODO Datetime: TODO Discrete: TODO Float: TODO String: TODO Int: TODO Missing %: Yes (TODO) Special NA value: TODO See uk_us_pf_household for more details. 詳情請見 uk_us_pf_household。 us_census_1940 Name: Version 8.0 Extract of 1940 Census full-count dataset Alias: US Census Subject Area: Social Science Precision: 1 Person 1 records Columns: 15 Continuous: TODO Datetime: TODO Discrete: TODO Float: TODO String: TODO Int: TODO Missing %: TODO (TODO) Special NA value: TODO Any IPUMS USA Full Count data is not redistribute without permission, so we set as “Private” 未經許可，不得重新分發任何 IPUMS USA 全統計資料，故設定為 “Private”。 https://usa.ipums.org/usa/1940CensusDASTestData.shtml Data summary Subject Area Subject Area refers to the classification of datasets based on the UC Irvine Machine Learning Repository. Subject Area 是根據加州大學爾灣分校機器學習資料庫的資料集分類。 Subject Area Counts Public Private Biology 1 iris   Business 3 car_insurance, ds_salaries, expedia_hotel_logs,   Climate and Enviorment 1 covtype   Computer Science 1 intrusion   Engineering 0     Games 0     Health and Medicine 5 nhanes_diabetes, smoking_driking, uk_us_pf_household, uk_us_pf_person, uk_us_pf_prop_net   Law 0     Physics and Chemistry 0     Social Science 3 adult, coil2000 us_census_1940 Other 0     car_insurance be categorized in Business, but coil2000 from UCI ML have been categorized in Social Science. Precision Precision Counts Public Private By Connection 2 intrusion, uk_us_pf_prop_net   By Geospatial 1 covtype   By House 1 uk_us_pf_household   By Organism 1 iris   By Person All remains (skip) (skip) By Recommendation 1 expedia_hotel_logs   Inclusion Reasons The inclusion of the Benchmark dataset is based on retaining only the most significant reason. 納入基準資料集原因只保留最重要的一個原因。 Reason Counts Public Private Common PETs 1 adult   Common DS/ML 2 coil2000, iris   Taiwan guidelines 1 nhanes_diabetes   112 ITRI 1 ds_salaries, smoking_driking   Precision 3 covtype, expedia_hotel_logs, uk_us_pf_person, uk_us_pf_prop_net   Replication 1   us_census_1940 Competition 1 uk_us_pf_household   Others 1 alarm, car_insurance, intrusion   Common PETs：Commonly Used in PETs fields 常用於隱私強化技術領域 Common DS/ML：Commonly Used in Data Science/Machine Learning 常用於資料科學/機器學習領域 The value of “iris” in privacy protection research is questionable “iris” 在隱私保護研究的價值是存疑的 Taiwan guidelines: The Taiwan guideline handbook has been utilized 臺灣指引手冊有使用 PETsWork: included “nhanes_diabetes” 112 ITRI: (Industrial Technology Research Institute) 工研院112年計畫成果有使用 Some of the data from 112 ITRI are solely test data created by the Academia Sinica, and the dataset size is too small so we excluded. 有些工研院112年使用的只是中研院產生的測試用假資料，且資料大小太小，故不納入。 Included fake_job.csv, fake_lat.csv, fake_lon.csv, revenue_tw_id.csv, sports_id.csv, and zh_tw_header.csv. Precision: Enrich Precision type 增加多元精度類型 The value of covtype in privacy protection research is questionable covtype 在隱私保護研究的價值是存疑的 Replication: Replicated research findings 重製研究成果 Competition: Used in a competition 在隱私強化技術競賽用過 Others: Others/Uncategorized 其他/未分類 some of SDGym datasets have be categorized here because not sure are these datasets popular enough. 有些 SDGym 的資料集被分在此，是由於不清楚這些資料集是否夠知名。 Mention in Research Topic Paper Counts Public Private Coverage% Notes Anonymeter 1. 2 adult us_census_1940 66.7% a. SDGym 2. 6 adult, alarm, car_insurance, covtype, expedia_hotel_logs, intrusion   66.7% b.c. smartnoise 3. 1 iris   16.7% d.e. Giomi, M., Boenisch, F., Wehmeyer, C., &amp; Tasnádi, B. (2023). A Unified Framework for Quantifying Privacy Risk in Synthetic Data. Proceedings of Privacy Enhancing Technologies Symposium, 2023(2), 312–328. https://doi.org/10.56553/popets-2023-0055 https://docs.sdv.dev/sdgym/customization/datasets/public-sdv-datasets https://github.com/opendp/smartnoise-sdk/tree/main/datasets a. texas requires payment. texas 需要付費。 b. census is duplicated and its webpage is offline. census 資料集重複，且其網頁已經無法訪問。 c. child and news datasets didn’t have reference. child 和 news 資料集缺乏參考資料。 d. 4 pums datasets discussion see below. 有關4個 pums 資料集的討論請參見下文。 e. reddit dataset didn’t have reference. reddit 資料集缺乏參考資料。 Used in Competition Topic Link Counts Public Private Notes PWSCup 1.2. 2 adult, nhanes_diabetes     UK-US 3. 3 uk_us_pf_household, uk_us_pf_person, uk_us_pf_prop_net     PWSCup2021: Use nhanes_diabetes https://www.iwsec.org/pws/2021/index.html PWSCup2020: Use Synthetic data from adult https://www.iwsec.org/pws/2020/cup20.html UK-US PETs prize challenges 2023: https://petsprizechallenges.com/ There’s couples of datasets in UK-US, we include only the most representative. Here’s remains: TODO Appendix for non-included datasets pums Name: The American Community Survey (ACS) Public Use Microdata Sample (PUMS) OpenDP’s and smartnoise’s demo use 1,000 records of California demographics, only below columns: [“age”, “sex”, “educ”, “race”, “income”, “married”]. But didn’t specific which year they use (2005~2022 is available) In reference to the NIST CRC 2023 Deidentified Data Archives, it may be possible to define multiple PUMAs based on geographical regions, and the classification approach can initially follow their guidelines OpenDP 與 smartnoise 的示範使用了 1,000 條加州人口統計數據，僅包含以下欄位：[“age”, “sex”, “educ”, “race”, “income”, “married”]。但並未具體指明使用了哪一年的數據（2005年至2022年的數據均可用）。 參考了 NIST CRC 2023 去識別化資料存檔的介紹，或許可以依照地區定義多個 PUMAs，然後分類方法可以先依照他們。 https://www.census.gov/programs-surveys/acs/microdata.html https://github.com/opendp/smartnoise-sdk/tree/main/datasets https://pages.nist.gov/privacy_collaborative_research_cycle/pages/archive.html#acceleration-bundle https://github.com/usnistgov/SDNist/tree/main/nist%20diverse%20communities%20data%20excerpts texas Name: Texas Hospital Discharge Data Public Use Data For data downloads by non-Texas public universities and public health institutions, it is necessary to complete an application form and pay an annual subscription fee. 對於非德州的公立大學和公共衛生機構來說，需要填寫一份申請表格並支付年度訂閱費。 https://www.dshs.texas.gov/texas-health-care-information-collection/general-public-information/hospital-discharge-data-public storage The module of benchmark dataset will first download the requested raw data, and store it in a “benchmark” folder within the working directory (in lowercase). If the folder does not exist, it will be created automatically (./benchmark/{Benchmark filename}). Subsequently, it will follow the regular Loader process for loading. When using it, please be mindful of your permissions and available hardware space. If the “benchmark” folder already contains a file with the same filename as the dataset, the program will check if the local data matches the records in PETsARD. If they match, the program will skip the download and use the local data directly, making it convenient for users to reuse the data multiple times. It’s important to note that if there is a file with the same name but with inconsistent data, Loader will issue a warning and stop. In such cases, users should be aware that the benchmark dataset might have been tampered with, potentially contaminating the experimental results. 基準資料集功能會先下載你所請求的原始資料，存到工作目錄下方的 “benchmark” 資料夾裡（小寫），如果不存在會自動開一個 (./benchmark/{Benchmark filename})，之後照一般的 Loader 流程加載。使用時請注意你的權限與硬體空間。 如果你的 “benchmark” 資料夾裡面已經有該資料集對應的同名檔案了，則程式會檢驗本地端的資料是否與 PETsARD 的紀錄一致，如果一致的話，便會省去下載、直接使用本地端資料，方便使用者多次使用。要注意的是如果同檔名但檢驗不一致的話，Loader 會告警並停止，此時使用者應該留意到可能儲存到了非原版的基準資料集，這很有可能對實驗結果造成汙染。 verification Classic benchmark datasets are often used in various data analysis or machine learning scenarios. However, when discussing the same dataset, it is common to find inconsistencies in the content in practical experience. Common patterns include: Inconsistent variable encoding transformations (e.g., having original categorical variables recorded as strings, Label Encoding encoded versions, or generalized categorical versions). Inconsistent row counts (e.g., versions before or after removing missing values). Inconsistent columns (e.g., column renaming, or versions after feature engineering). The reasons for these patterns are often not malicious tampering, but rather the release of optimized or preprocessed data, which is then inadvertently propagated by subsequent users. Since the preprocessing methods for privacy-enhancing technologies are crucial, obtaining the same version of the benchmark dataset is the recommended experimental procedure in PETsARD. 經典的基準資料集常被用於各種資料分析或機器學習的場合，但實務經驗上，常遇到討論同一個資料集的時候，發現彼此的資料集內容不一致。常見的樣態有： 變項編碼轉換不一致（例如分別有原始以字串紀錄的類別變項、Label Encoding過的編碼、概化後的分類的版本） 筆數不一致（例如剔除遺失值前或後的版本） 欄位不一致（例如欄位重命名，或是欄位經過特徵工程的版本） 造成這些樣態原因常常不是惡意竄改，而是某些優化過、或是前處理過的資料被釋出，而後續使用者不經意地加以傳播所導致。由於隱私強化技術的前處理方式至關重要，於是取得相同版本的基準資料集是 PETsARD 建議的實驗程序。 _calculate_sha256() PETsARD ensures the consistency of benchmark datasets’ versions by calculating the SHA-256 hash value of files and comparing it to the settings in the benchmark_datasets.yaml file. Specifically, PETsARD uses hashlib.sha256() to calculate the hash of the binary version of a file and records its hexadecimal representation (.hexdigest()). In the functionality related to benchmark datasets, the SHA-256 calculation is entirely background, automated, and in future versions, methods will be provided for users to calculate it manually if desired. PETsARD 藉由計算檔案的 SHA-256 值，與設定檔案 benchmark_datasets.yaml 做比較，來確保基準資料集的版本一致。具體來說，PETsARD 使用 hashlib.sha256() 計算檔案二進位版本的雜湊，並記錄其雜湊值的 16 進位表示 (.hexdigest())。 在基準資料集的功能中，計算 SHA-256 是完全後台的、自動化的，未來版本會再提供方法供使用者自行計算。 public/private access The PETsARD development team, which belongs to the National Institute for Cyber Security (NICS), stores the benchmark datasets in their cloud space. These datasets are categorized as public or private access, and the calling methods for both are identical. Public datasets are securely stored in the cloud space after obtaining proper authorization, and they can be downloaded using request.get() On the other hand, Private datasets, which may have restrictions based on the dataset’s authorization or considerations from the data provider, are intended for internal use by the development team and collaborating parties. Access to private datasets is established through boto3 connection, and configuring cloud permissions is necessary. For any related inquiries, please contact the development team. 基準資料集儲存在 PETsARD 開發團隊所屬的臺灣國家資通安全研究院 (NICS)雲端空間，分成公開與私有訪問兩種。兩者的呼叫方式完全一樣。 公開資料集皆是確認過其授權後，由團隊保存之雲端備份，使用 request.get() 下載。而私有資料集，包含了資料集本身授權的限制、或是資料提供方的考量等原因，僅供團隊與合作方內部使用。目前私有資料集使用 boto3 連線，需要預先設定雲端權限，相關問題請聯絡開發團隊。"
  },"/PETsARD-Gitbook/Scaler.html": {
    "title": "Scaler",
    "keywords": "",
    "url": "/PETsARD-Gitbook/Scaler.html",
    "body": "Scaler The Scaler module is designed to standardise and scale data using various methods. Four scaling methods are provided: Scaler 模組旨在使用各種方法對數據進行標準化和縮放。套件中提供了四種縮放方法： Scaler_Standard: Standardise the data This method applies StandardScaler from the sklearn library, transforming the data to have a mean of 0 and a standard deviation of 1. 此方法使用 sklearn 中的 StandardScaler，將資料轉換為平均值為 0、標準差為 1 的樣態。 from PETsARD.Processor.Scaler import Scaler_Standard scaler = Scaler_Standard() Scaler_ZeroCenter: Scale the data to have the mean=0 Utilising StandardScaler from sklearn, this method centres the transformed data around a mean of 0. 利用 sklearn 中的 StandardScaler，將資料轉換為平均值為 0 的樣態。 from PETsARD.Processor.Scaler import Scaler_ZeroCenter scaler = Scaler_ZeroCenter() Scaler_MinMax: Scale the data to have the range [0, 1] By applying MinMaxScaler from sklearn, this method scales the data to fit within the range [0, 1]. 利用 sklearn 中的 MinMaxScaler，將資料轉換至 [0, 1] 的範圍。 from PETsARD.Processor.Scaler import Scaler_MinMax scaler = Scaler_MinMax() Scaler_Log: Scale the data by log transformation This method requires the input data to be positive. It applies log transformation to mitigate the impact of extreme values. 此方法僅能在資料為正的情形可用，可用於減緩極端值對整體資料的影響。 from PETsARD.Processor.Scaler import Scaler_Log scaler = Scaler_Log()"
  },"/PETsARD-Gitbook/Processor.html": {
    "title": "Processor",
    "keywords": "",
    "url": "/PETsARD-Gitbook/Processor.html",
    "body": "Processor The Processor module is responsible for managing preprocessing and postprocessing procedures during experiments. This component facilitates easy data handling, including tasks such as encoding categorical data, handling missing data, excluding outliers, and scaling data. This guide will walk you through the creation and manipulation of a processor instance from the Processor class. Processor 模組負責在實驗期間管理資料前處理和後處理（還原）的過程。此元件可進行多種數據處理，包括為類別資料進行編碼、處理缺失值、排除異常值以及標準化資料等任務。本指南將引導您建立和操作 Processor 類的物件。 from PETsARD.Processor.Base import Processor processor = Processor(metadata) processor.fit(data) transformed_data = processor.transform(data) inverse_transformed_data = processor.inverse_transform(synthetic_data) Processor Creation Once you have an instance of metadata built from the Metadata class, you can create a Processor. The config parameter is optional, allowing you to customise procedures. Upon creation, the processor analyses the metadata to determine the necessary preprocessing and postprocessing procedures. If a config is passed, the processor will overwrite default settings and follow the procedures specified in the config. 創建 Processor 類別的物件之前，必須要有利用 Metadata 建立的 metadata 物件。在 Processor 參數中，config 參數不是必須的，其功能為自訂處理流程。此物件會分析 metadata 以確定所需的前處理和後處理流程。如果有給予 config，物件會覆寫預設值，並依照 config 中自訂的流程執行。 processor = Processor( metadata, # required config=None ) Parameters metadata: The data schema used for creating the processor and inferring appropriate data processing procedures. config (dict): User-defined procedures containing information about the components to be used in each column. metadata: 用於推論前處理及後處理流程的數據架構。 config (dict): 針對每個欄位的自定義處理流程。 get_config Use this method to access the configuration of procedures to be done during the transformation/inverse transform process. It is summarised by the processor types (missingist, outlierist, encoder, scaler) and columns, storing all data processing objects for user access. 使用此方法取得在轉換/逆轉換過程中的設定檔。此設定檔依據處理類型（missingist、outlierist、encoder、scaler）與欄位進行整理，並呈現給使用者使用，使用者可以直接透過此方法存取儲存在內的處理物件。 processor.get_config( col=None, print_config=False ) {'missingist': {'gen': &lt;PETsARD.Processor.Missingist.Missingist_Drop at 0x14715dcc0&gt;, 'age': &lt;PETsARD.Processor.Missingist.Missingist_Simple at 0x14715f9d0&gt;, }, 'outlierist': {'gen': None, 'age': &lt;PETsARD.Processor.Outlierist.Outlierist_LOF at 0x14715c670&gt;, }, 'encoder': {'gen': &lt;PETsARD.Processor.Encoder.Encoder_Uniform at 0x14715c1f0&gt;, 'age': None }, 'scaler': {'gen': None, 'age': &lt;PETsARD.Processor.Scaler.Scaler_MinMax at 0x14715d300&gt; } } Parameters col (list): The columns the user wants to get the config from. If the list is empty, all columns from the metadata will be selected. print_config (bool, default=False): Whether the result should be printed. col (list): 欲取用的欄位。若沒有輸入則視為選擇所有的欄位。 print_config (bool, default=False): 是否需列印結果。 Outputs (dict): The config with selected columns. (dict): 含有選定欄位的設定檔。 set_config Edit the whole config. To maintain the structure of the config, it fills the unspecified processors with None. If you don’t want to do this, use update_config instead. 編輯整份設定檔。為了保持設定檔的結構，會將未指定的處理器設為 None。如果您不想這樣做，請改用 update_config。 processor.set_config(config) Parameters config (dict): The dict with the same format as the config class. config (dict): 與設定檔格式相同的 dict 輸入。 update_config Update part of the config. 更改部分設定檔。 processor.update_config(config) Parameters config (dict): The dict with the same format as the config class. config (dict): 與設定檔格式相同的 dict 輸入。 get_changes Compare the differences between the current config and the default config. 比較目前設定檔與預設設定檔之間的差異。 processor.get_changes() Outputs (pandas.DataFrame): A dataframe recording the differences bewteen the current config and the default config. (pandas.DataFrame): 記錄兩者差異的資料表。 Data Processing fit Learn the structure of the data. 學習資料整體結構。 processor.fit( data, sequence=None ) Parameters data (pandas.DataFrame): The data to be fitted. sequence (list): The processing sequence, allowing users to skip procedures and alter the execution order. Avaliable procedures: ‘missingist’, ‘outlierist’, ‘encoder’, ‘scaler’. This is the default sequence if the user doesn’t pass a sequence to the method. data (pandas.DataFrame): 用來學習的資料。 sequence (list): 處理流程，可允許用戶跳過特定流程或改變執行順序。可用的流程選項：’missingist’、’outlierist’、’encoder’、’scaler’。若用戶未指定流程，則使用此作為預設序列。 transform Conduct the data preprocessing procedure. 進行資料前處理。 transformed = processor.transform(data) Parameters data (pandas.DataFrame): The data to be transformed. data (pandas.DataFrame): 要轉換的資料。 Outputs (pandas.DataFrame): The data after transformation. (pandas.DataFrame): 轉換完成的資料。 inverse_transform Conduct the data postprocessing procedure. 進行資料後處理。 inverse_transformed = processor.inverse_transform(data) Parameters data (pandas.DataFrame): The data to be inverse transformed. data (pandas.DataFrame): 要轉換的資料。 Outputs (pandas.DataFrame): The data after inverse transformation. (pandas.DataFrame): 轉換完成的資料。"
  },"/PETsARD-Gitbook/Outlierist.html": {
    "title": "Outlierist",
    "keywords": "",
    "url": "/PETsARD-Gitbook/Outlierist.html",
    "body": "Outlierist The Outlierist module is designed to identify and remove data classified as outliers. Four methods for identifying outliers are provided: Outlierist 模組旨在識別並刪除被歸類為異常值的數據。此套件提供了四種識別異常值的方法： Outlierist_ZScore: Identify outliers by z-score This method classifies data as outliers if the absolute value of the z-score is greater than 3. 此方法將 z 分數的絕對值大於 3 的資料歸類為異常值。 from PETsARD.Processor.Outlierist import Outlierist_ZScore outlierist = Outlierist_ZScore() Outlierist_IQR: Identify outliers by IQR Data outside the range of 1.5 times the interquartile range (IQR) is determined as an outlier. 在此方法中，超過 1.5 倍四分位距（IQR）範圍的資料會被視為異常值。 from PETsARD.Processor.Outlierist import Outlierist_IQR outlierist = Outlierist_IQR() Outlierist_IsolationForest: Identify outliers by Isolation Forest This method uses IsolationForest from sklearn to identify outliers. It is a global transformation, meaning that if any column uses the isolation forest as an outlierist, it will overwrite the entire config and apply isolation forest to all outlierists. 此方法使用 sklearn 的 IsolationForest 進行異常值識別。這是一種全域轉換，意即只要設定檔中有任何欄位使用此方法作為異常值處理器，它將覆寫整個設定檔並將此方法應用於所有欄位。 from PETsARD.Processor.Outlierist import Outlierist_IsolationForest outlierist = Outlierist_IsolationForest() Outlierist_LOF: Identify outliers by Local Outlier Factor This method uses LocalOutlierFactor from sklearn to identify outliers. It is a global transformation, meaning that if any column uses the isolation forest as an outlierist, it will overwrite the entire config and apply isolation forest to all outlierists. 此方法使用 sklearn 的 LocalOutlierFactor 進行異常值識別。這是一種全域轉換，意即只要設定檔中有任何欄位使用此方法作為異常值處理器，它將覆寫整個設定檔並將此方法應用於所有欄位。 from PETsARD.Processor.Outlierist import Outlierist_LOF outlierist = Outlierist_LOF()"
  },"/PETsARD-Gitbook/Missingist.html": {
    "title": "Missingist",
    "keywords": "",
    "url": "/PETsARD-Gitbook/Missingist.html",
    "body": "Missingist The Missingist module handles missing values in a dataset, offering four methods for coping with them. Missingist 模組處理數據集中的缺失值，並提供四種處理方法。 Missingist_Drop: Drop the missing values This method involves dropping the rows containing missing values in any column. 捨棄任何含有缺失值的列。 from PETsARD.Processor.Missingist import Missingist_Drop missingist = Missingist_Drop() Missingist_Mean: Fill the missing values with the mean Missing values are filled with the mean value of the corresponding column. 將缺失值用該欄的平均值填入。 from PETsARD.Processor.Missingist import Missingist_Mean missingist = Missingist_Mean() Missingist_Median: Fill the missing values with the median Missing values are filled with the median value of the corresponding column. 將缺失值用該欄的中位數填入。 from PETsARD.Processor.Missingist import Missingist_Median missingist = Missingist_Median() Missingist_Simple: Fill the missing values with a predefined value Missing values are filled with a predefined value for the corresponding column. 將缺失值用指定的值填入。 from PETsARD.Processor.Missingist import Missingist_Simple missingist = Missingist_Simple(value=0.0) Parameters value (float): The value to be imputed. value (float): 要填入的自訂值。"
  },"/PETsARD-Gitbook/Encoder.html": {
    "title": "Encoder",
    "keywords": "",
    "url": "/PETsARD-Gitbook/Encoder.html",
    "body": "Encoder The Encoder module transforms categorical data into numerical format, a requirement for many modeling procedures. Currently, we provide two encoders: the uniform encoder and the label encoder. Encoder 模組將類別資料轉換為連續型資料，方便套用大多數的模型。目前我們提供兩種方法：Uniform encoder 和 Label encoder。 Encoder_Uniform: Uniform Encoder Applying uniform encoders during data processing, as suggested by datacebo, can enhance the performance of generative algorithms compared to other encoders. The concept is straightforward: map each category to a specific range in the uniform distribution, with ranges determined by the relative proportion of each category in the data. Major categories occupy larger areas under the distribution. datacebo 認為在資料處理過程中使用 Uniform encoder 來處理類別資料，可以提升生成模型的表現。Uniform encoder 的概念非常直觀：將每個類別映射到 Uniform distribution 中的特定範圍，範圍由資料中各類別的比例決定，因此較常見的類別會對應到較大的範圍。 Advantages of using a uniform encoder: The variable’s distribution converts from discrete to continuous, facilitating modeling. The range of the new distribution is fixed, allowing easy conversion of any value between 0 and 1 to a category. The mapping relationship retains information about the original distribution, a valuable property for sampling. More frequent categories are more likely to be sampled due to their larger areas under the distribution. 相較於其他類型的處理方式，使用 Uniform encoder 有以下優勢： 變數的分布從離散轉換為連續，有助於建模。 新分配的範圍固定，可將任何介於 0 和 1 之間的值輕鬆轉換為類別變數。 映射關係保留原始資料分配的訊息，有助於進行抽樣。由於出現頻率較高的類別其分配下的面積較大，因此更有可能被抽樣，反映出原始資料的樣態。 A toy example demonstrates the output of a uniform encoder: Assuming a categorical variable with three categories, ‘a’, ‘b’, and ‘c’, and associated proportions of 1:3:1, respectively. The mapping relationship is as follows: { 'a': [0.0, 0.2), 'b': [0.2, 0.8), 'c': [0.8, 1.0] } After transformation by the uniform encoder, data belonging to category ‘a’ will be assigned a random value between 0.0 (inclusive) and 0.2 (exclusive), data in category ‘b’ between 0.2 (inclusive) and 0.8 (exclusive), and data in category ‘c’ between 0.8 (inclusive) and 1.0 (inclusive). To inverse transform numerical data to categorical data, simply check the range in which the value falls and convert it back to the corresponding category using the mapping relationship. 以下是 Uniform encoder 的簡易範例： 假設一個具有三個類別 ‘a’、’b’ 和 ‘c’的類別變數，其資料比例為 1:3:1。則映射關係如下： { 'a': [0.0, 0.2), 'b': [0.2, 0.8), 'c': [0.8, 1.0] } 經過 Uniform encoder 的轉換後，類別 ‘a’ 會用介於 0.0（包含）和 0.2（不包含）之間的隨機值取代，類別 ‘b’ 會用介於 0.2（包含）和 0.8（不包含）之間的隨機值取代，類別 ‘c’ 則會用介於 0.8（包含）和 1.0（包含）之間的隨機值取代。 而要將連續型變數反轉為類別資料，只需檢查其值所處的範圍，然後使用映射關係將其轉換回相應的類別即可。 Uniform encoders are available by calling Encoder_Uniform. 要使用此方法，可以透過 Encoder_Uniform 類別。 from PETsARD.Processor.Encoder import Encoder_Uniform encoder = Encoder_Uniform() Encoder_Label: Label Encoder Transform categorical data into numerical data by assigning a series of integers (1, 2, 3,…) to the categories. 將類別變數對應到一系列的整數 (1, 2, 3,…) 藉此達到轉換為連續型資料的目的。 from PETsARD.Processor.Encoder import Encoder_Label encoder = Encoder_Label()"
  },"/PETsARD-Gitbook/Evaluator-Anonymeter.html": {
    "title": "Evaluator anonymeter",
    "keywords": "",
    "url": "/PETsARD-Gitbook/Evaluator-Anonymeter.html",
    "body": "Anonymeter Anonymeter is a comprehensive Python library that evaluates various aspects of privacy risks in synthetic tabular data, including Singling Out, Linkability, and Inference risks. These three points are based on the criteria established by Article 29 Data Protection Working Party (WP29) under the Data Protection Directive (Directive 95/46), as outlined in their written guidance published in 2014, for evaluating the effectiveness standards of anonymization techniques. Anonymeter` received positively reviewed from the Commission Nationale de l’Informatique et des Libertés (CNIL) on February 13, 2023, acknowledging its ability to effectively evaluate the three standards of anonymization effectiveness in synthetic data. CNIL recommends using this library to evaluate the risk of re-identification. Therefore, PETsARD includes built-in calls to Anonymeter. For more details, please refer to its official GitHub: statice/anonymeter Anonymeter 是一個全面評估合成表格資料中不同層面隱私風險的 Python 函式庫，包括指認性(Singling Out)、連結性(Linkability)、和推斷性(Inference)風險。 此三點是根據歐盟個人資料保護指令第29條設立之個資保護工作小組(WP29)，於 2014 年發布的書面指引中所列出，用於評估匿名化技術的有效性標準。而 Anonymeter 於 2023 年 02 月 13 日受到法國國家資訊自由委員會(CNIL)的正面評價，認為此工具能有效評估合成資料的匿名化有效性三個標準，並建議使用本函式庫來評估資料被重新識別的風險。 因此 PETsARD整合了對 Anonymeter 的使用。更多詳情請參閱其官方 GitHub：statice/anonymeter from PETsARD import Evaluator eval = Evaluator( evaluating_method='anonymeter-singlingout-univariate', data={'ori': train_data, 'syn': synethsizing_data, 'control': validation_data } ) eval.eval() print(eval.Evaluator.evaluation) {'Risk': 0.0, 'Risk_CI_btm': 0.0, 'Risk_CI_top': 0.0013568577126237004, 'Attack_Rate': 0.0009585236406264672, 'Attack_Rate_err': 0.0009585236406264671, 'Baseline_Rate': 0.0009585236406264672, 'Baseline_Rate_err': 0.0009585236406264671, 'Control_Rate': 0.0009585236406264672, 'Control_Rate_err': 0.0009585236406264671} Common: Inherited from Evaluator In the Anonymeter module, which is embedded within the Evaluator module and inherits parameters from it. Additionally, “Anonymeter” follows a standardized output format. Anonymeter 內嵌在 Evaluator 模組、並繼承其參數。同時 Anonymeter 也有統一的輸出格式。 evaluating_method evaluating_method (str): evaluating method 評估方法 The parameter evaluating_method, which is inherited from Evaluator, determines which evaluation module to invoke. Input values starting with ‘anonymeter-‘ can call the following methods of Anonymeter (case-insensitive): 繼承自 Evaluator 的參數評估方法 (evaluating_method) 能決定呼叫哪種評估模組，其中以 ‘anonymeter-‘ 開頭的輸入值便能調用 Anonymeter，有以下方法（大小寫不分）： ‘anonymeter-singlingout-univariate’: Singling Out risk - Univariate mode 指認性風險 - 單變數模式 ‘anonymeter-linkability’: Linkability risk 連結性風險 ‘anonymeter-inference’: Inference risks 推斷性風險 evaluating_method='anonymeter-singlingout-univariate' # Singling Out risk evaluating_method='anonymeter-linkability' # Linkability risk evaluating_method='anonymeter-inference' # Inference risk data data (Dict[str, pd.DataFrame]): input data 輸入資料 data is a dictionary with a specific format, containing specific keys, and each value is a Pandas DataFrame. data 是具特定格式的字典，有特定的鍵，並且每個值都是一個 Pandas DataFrame。 ori: Original Train data. A portion of the original data used to generate synthetic data (syn). syn: Synethsizing data. Synthetic data generated from ori. control: Validation data. Another portion of the original data not used for generating synthetic data and kept confidential. ori：原始訓練資料。被用於生成合成資料 (syn )的原始資料部份。 syn：合成資料。是用 ori 生成得來。 control：驗證資料。控制未用於生成合成資料、訊息未洩漏的原始資料部份。 n_attacks n_attacks (int, default=2000): Number of target records for specific attack 特定攻擊的攻擊目標數 n_attacks is the parameter in Anonymeter that specifies how many times this particular attack will be executed. A higher number will reduce the statistical uncertainties on the results, at the expense of a longer computation time. In fact, each type of attack has a potential maximum limit on number of attacks: Singling Out: Number of distinct queries. A query is a specific condition-based searching command matching only one record in a certain field, achieving Singling Out. Linkability and Inference: Number of rows in the training dataset. The implementation of these two attack methods is to sample from the Original Train data. If a value exceeding the potential maximum attack count is set, Anonymeter will issue a warning and disregard the remaining counts. Therefore, we plan to determine the default upper limit automatically in future release, to ensure comprehensive testing for PETsARD. If users find that the computation time is too long during a trial, it is recommended to reduce reduce it to the attack numbers provided in the official Anonymeter examples: 500 for Singling Out, and 2,000 for Linkability and Inference. n_attacks 代表著 Anonymeter 將執行這種攻擊多少次的參數，較高的數量會降低結果的統計不確定性，但會增加運算時間。事實上各種攻擊方式都存在有潛在的攻擊次數上限： Singling Out：不重複搜索語句 (queries)的數量。搜索語句是特定的條件查詢式，使得該語句能在某欄位中僅對應到一筆資料，達到指認性。 Linkability 跟 Inference：訓練資料集行數。這兩種攻擊方式背後實現的原理，是對原始訓練資料進行抽樣。 如果設定了超過潛在上限攻擊次數的值，則 Anonymeter 將回傳警告，並忽略剩下的次數。 因此，在未來的更新中，我們計劃能預設自動判斷攻擊的上限，以確保 PETsARD 的測試足夠全面。如果使用者在試用中發現運算時間過長，可以暫時將攻擊數調低，而 Anonymeter 官方範例中設定的值為：Singling Out 為 500，Linkability 跟 Inference 為 2,000。 .eval() Execute the evaluation. This method is inherited from Evaluator, and does not require nor accept any parameters.” 執行評估。繼承自 Evaluator 的方法，不需也不接受任何參數。 eval = Evaluator(...) eval.eval() output self.Evaluator.evaluation (Dict[str, float]): evaluation 評估結果 The evaluation results are stored directly as a dictionary in self.Evaluator.evaluation with a specific format, and all values are floating-point numbers within the range of 0.0 to 1.0: 評估結果直接作為字典儲存在 self.Evaluator.evaluation 內，具有特定格式，且值都是範圍為 0.0 ~ 1.0 的浮點數： {'Risk': 0.0, 'Risk_CI_btm': 0.0, 'Risk_CI_top': 0.0, 'Attack_Rate': 0.0, 'Attack_Rate_err': 0.0, 'Baseline_Rate': 0.0, 'Baseline_Rate_err': 0.0, 'Control_Rate': 0.0, 'Control_Rate_err': 0.0} key Definition 定義 Risk Privacy Risk 隱私風險 Risk_CI_btm The bottom of confidence interval of Privacy Risk 隱私風險信賴區間下界 Risk_CI_top The top of confidence interval of Privacy Risk 隱私風險信賴區間上界 Attack_Rate The Main Privacy Attack rate 主要隱私攻擊率 Attack_Rate_err Error of Main Privacy Attack rate 主要隱私攻擊率誤差 Baseline_Rate The Baseline Privacy Attack rate 基線隱私攻擊率 Baseline_Rate_err Error of the Baseline Privacy Attack rate 基線隱私攻擊率誤差 Control_Rate The Control Privacy Attack rate 控制隱私攻擊率 Control_Rate_err Error of the Control Privacy Attack rate 控制隱私攻擊率誤差 Privacy Risk is a high level estimation of specific privacy risk obtained from the attack rates mentioned below. Its formula is as follows. The numerator represents the attacker’s exploitation of synthetic data, as the Main Attack to excess of the Control Attack success rate. The denominator is the normalization factor by 1 minus Control Attack, representing the signifying the effectiveness of Main Attack relative to the Perfect Attacker (100%), to calculate the difference in the numerator. Perfect Attacker is a concept that represents an all-knowing, all-powerful attacker. In our evaluating, this means they have a 100% chance of a successful attack. Therefore, the underlying idea behind this score is that Main Attack, due to their access to synthesized data, have a higher success rate compared to Control Attack. However, the proportion of this success rate increase relative to the Perfect Attacker’s perfect success rate is what matters. Ranging from zero to one, with higher numbers indicating higher privacy risk, the information provided by synthetic data brings attackers closer to that of a perfect attacker. 隱私風險是綜合下述攻擊率而得到的對特定隱私風險的評估，其公式如下。 分子代表攻擊者利用合成資料的攻擊、也就是主要攻擊對控制攻擊成功率的改進。 分母則以 1 減 控制攻擊 代表主要攻擊相對於完美攻擊者 (100%) 的效果，作為歸一化因子計算分子的差異。 完美攻擊者是一個概念，代表著一個全知全能的攻擊者，在我們的驗測中，這表示他有 100% 的成功攻擊機會。因此，這個分數背後的思想是，主要攻擊因為取得合成資料，因此相對於控制攻擊有更高的成功率，但這個成功率提升，相對於完美攻擊者完美的成功率提升，所佔的比例有多少。 零到一，數字越大代表隱私的風險越高，合成資料提供的資訊能使攻擊者越接近完美攻擊者。 \\[PrivacyRisk = \\frac{AttackRate_{Main}-AttackRate_{Control}}{1-AttackRate_{Control}}\\] Attack Rate refers to the proportion of successful executions of a specific attack, whether by malicious or honest-but-curious users. Also called Success Attack Rate. Since it is assumed that each attack is independent, and attacks are only concerned with either success or failure, they can be modeled as Bernoulli trials. The Wilson Score Interval can be used to estimate the binomial success rate and adjusted confidence interval as below. The default of confidence level is 95%. From zero to one, a higher number indicates a higher success rate for that specific attack. 攻擊率意指無論是由惡意還是誠實但好奇的使用者成功執行特定攻擊的比例。又被稱為成功攻擊率。 由於假設每次攻擊都是獨立的，而攻擊只關心成功或失敗兩種結果，因此它們可以被建模為伯努利試驗。可以使用威爾遜分數區間來估算二項式成功率與調整後的信賴區間如下。預設信心水準為 95%。 零到一，數字越大代表該特定攻擊的成功率越高。 \\[AttackRate = \\frac{N_{ Success}+\\frac{ {Z}^{2} }{2} }{ N_{Total}+{Z}^{2} }\\quad\\left \\{\\begin{matrix} N_{Success} &amp; Number\\;of\\;Success\\;Attack\\\\ N_{Total} &amp; Number\\;of\\;Total\\;Attack\\\\ Z &amp; Z\\;score\\;of\\;confidence\\;level \\end{matrix} \\right.\\] Main Attack Rate refers to the attack rate inferred from the training data records using synthetic data. Baseline Attack Rate or Naive Attack Rate is the success rate inferred from the training data records using random guessing. The Baseline Attack Rate provides a benchmark for measuring the strength of attacks. If the Main Attack Rate is less than or equal to the Baseline Attack Rate, it indicates the Main Attack modeling is less effective than random guessing. In this case, the result is meaningless, and the Anonymeter library will issue a warning, suggesting that users should exclude such results from their analysis to avoid incorrectly reporting a “no risk” outcome. PETsARD will directly return the result, and users are responsible for their own filtering. The possibility of the Main Attack being inferior to random includes scenarios with insufficient attack occurrences (n_attacks), attackers having too little auxiliary information (e.g., misconfigured aux_cols in the Inference function), or issues with the data itself (e.g., too few columns, too few records, or too few combinations of categorical variables). Control Attack Rate is the attack rate inferred from the control data records using synthetic data. 主要攻擊率 (Main Attack Rate) 是指使用合成資料來推斷訓練資料紀錄的攻擊率 基線攻擊率 (Baseline Attack Rate) 或是天真攻擊率 (Naive Attack Rate) 則是使用隨機猜測來推斷訓練資料紀錄的成功率 基線攻擊率提供了衡量攻擊強度的基準值，如果主要攻擊率小於等於基線攻擊率，則代表主要攻擊的建模、其效果還不如隨機猜測，此時結果沒有意義，Anonymeter函式庫會在回傳結果的同時。警告用戶應該從分析中加以排除，避免錯誤的報告成「沒有風險」的結果。PETsARD 會直接回傳結果，請用戶自行篩選。 導致主要攻擊率不如隨機猜測的可能性，包括攻擊次數過少 (n_attacks)，攻擊者可獲得的輔助資訊過少（例如 Inference 功能中 aux_cols 設定錯誤），或者資料本身存在問題（例如欄位數量不足、記錄太少、或者類別變數的排列組合過於有限等情況）。 控制攻擊率 (Control Attack Rate) 則是使用合成資料來推斷控制資料紀錄的攻擊率 evaluating_method=’anonymeter-singlingout-univariate’ Singling Out risk represents the possibility of still being able to identify a particular individual, their part, or complete records, even after any Privacy-Enhancing Techniques have been applied. In the example from the Anonymeter, it refers to the scenario where “there is only one person with attributes X, Y, and Z”. In other words, attackers may attempt to identify specific individuals. The paper on Anonymeter specifically mentions: “It’s important to note that singling out does not imply re-identification. Yet the ability to isolate an individual is often enough to exert control on that individual, or to mount other privacy attacks.” Currently, only single variable mode of Singling Out evaluating (univariate) is implemented. In future updates, multi variables mode (multivariate) will be included to support singling out attacks by multiple attributes combination. 指認性風險表示即便經過隱私強化技術處理，仍有多大的可能性去識別出來特定個體，其部分或完整記錄的可能性。以 Anonymeter 的舉例，就是「只有一個人同時擁有著 X、Y、與 Z 特徵」。換句話說，攻擊者可以嘗試辨識出特定的個體。 Anonymeter 的論文有特別提到：「值得注意的是，指認不等於重新識別。然而，能夠單獨辨識一個體通常足以對該個體施加控制，或者進行其他隱私攻擊。」 目前僅實作單變數指認性驗測 (univariate)，未來更新將納入指認性驗測的多變數模式 (multivariate)，支援結合多種屬性的指認攻擊。 n_cols n_cols (int): Number of attributes used in the attacker queries 攻擊中所使用的屬性數量 Only applicable to multi-variable mode (multivariate), not implemented 僅適用於多變數模式 (multivariate)，未實作。 evaluating_method=’anonymeter-linkability’ Linkability risk represents the possibility that, even after Privacy-Enhancing Techniques have been applied, or when records exist in different databases, at least two records about the same individual or group of individuals can still be linked together. In the example from the Anonymeter, it refers to the scenario where “records A and B belong to the same person”. In particular, even if attackers cannot single out the specific individual’s identity, they may still attempt to establish links between records through shared features or information. 連結性風險表示即使經過隱私強化技術處理、或是存在不同的資料庫中，仍有多大的可能，將至少兩條關於同一個人或一組人的記錄連結在一起。以 Anonymeter 的舉例，就是「紀錄 A 與紀錄 B 屬於同一個人」。具體來說，即使攻擊者無法指認具體的個體身份，他們仍可能嘗試透過某些共同特徵或資訊，來建立記錄之間的關聯。 aux_cols aux_cols (Tuple[List[str], List[str]]) Columns of auxiliary information 輔助資訊欄位 The pattern of Linkability attacks assumes that attackers, whether malicious or honest-but-curious users, possesses two sets of non-overlapping original train data columns. When composite synthesized data involving these two sets of data columns is released, the attacker can use the synthetic data to link to their own original data, to determine whether the data from one dataset belongs to certain records in another dataset. In this context, the auxiliary data columns aux_cols are the two pieces of information the attackers own. For example, a medical center intends to release synthesized data from their heart disease research, which includes age, gender, postal code, and the number of heart attacks. Meanwhile, the attacker may have obtained real population data, such as gender and postal codes, from public sources or data leaks, along with real epidemiological data, such as age and the frequency of heart attacks, in their original form or in proportion. In this case, aux_cols` would be as follows: 連結性攻擊的攻擊樣態，是假定攻擊者，無論惡意還是誠實但好奇的使用者，擁有兩部分不重疊的原始訓練資料欄位，而當涉及這兩份資料欄位的綜合性合成資料被釋出，攻擊者便可以用合成資料連結到自己手中的原始資料，來推測哪些資料是互相對應的。此時輔助資料欄位 aux_cols 便是這兩批資料所各自包含的資料欄位。 舉例來說，某間醫學中心要釋出自己心臟病研究的合成資料，其中包括了年齡、性別、郵遞區號、心臟病發次數，而攻擊者可能已經從公開資料或資料洩漏中，得知了真實的戶政資料：性別與郵遞區號、以及真實的流行病學資料：年紀與心臟病發次數，兩種資料的比例或原始資料。那 aux_cols 便如下所示： aux_cols = [ ['sex', 'zip_code'], # public ['age', 'heart_attack_times'] # private ] The potential linkage attack method in this case may be that “due to the close similarity between the real population data and real epidemiological data with the values in this synthesized data, it is possible to link the age and the frequency of heart attacks of a certain group of people from the population data, or link the gender and place of residence of a certain group of people from the epidemiological data.” aux_cols involves domain-specific knowledge about the dataset, so neither PETsARD nor Anonymeter provide default values for it. Users need to configure it themselves based on their understanding of the dataset. In future updates, following the experimental approach outlined in the Anonymeter paper, different amounts of auxiliary information will be considered. The attacker’s auxiliary information will be sampled from “only two columns” to “the maximum number of columns in the dataset,” and these options will be provided as default values. 而此時潛在的連結性攻擊方式，便可能是「由於真實戶政資料跟真實流行病學資料，都跟此合成資料的數值差異足夠接近，於是可以由戶政資料連結出某群人的年紀與心臟病發次數，或是由流行病學資料連結出某群人的性別與居住地」。 aux_cols 涉及對資料集的專業知識，故 PETsARD 跟 Anonymeter 均不設預設值，須由使用者自行設定。在未來更新中，也將依照 Anonymeter 論文的實驗方式，考量不同數量的輔助資訊，將攻擊者的輔助資訊從「僅有兩列」到「資料集的最大列數」所有抽樣方式都遍歷考慮一次，提供這樣的預設值。 n_neighbors n_neighbors (int, default=10): The N closest neighbors considered for the link search 連結搜索時考慮的前 N 個最近鄰居數量 To handle mixed data types, Anonymeter uses Gower’s Distance/Similarity: Numeric variables: Gower’s Distance is the absolute difference between the normalized values. Categorical variables: Gower’s Distance is 1 if the values are not equal. After combining all attributes, the Manhattan Distance is calculated, and return the nearest N neighbors. So, in the context of Linkability risk, n_neighbors represents how close the two sets of data from the same person need to be linked to be considered a successful linkability attack. Anonymeter does not provide a recommended value for n_neighbors. In an example, a value of 10 is used, but the default value in the code is 1. PETsARD uses 10 as the default value. 為了處理混合資料類型的資料，Anonymeter 使用的是高爾距離/高爾相似性 (Gower’s Distance/Similarity)： 數值型變數：高爾距離為歸一化後兩者相差的絕對值 類別型變數：只要不相等，高爾距離即為 1 綜合所有屬性之後計算其曼哈頓距離，最後返回最近的 N 個鄰居。於是 n_neighbors 在 連結性風險 上的意思，是指同一個人的兩批資料，要在多近的距離內被連結到，才算是連結性攻擊成功。 Anonymeter 並沒有給出 n_neighbors 的建議值，在範例中使用 10、但在程式裡預設值是 1，PETsARD 使用 10 作為預設值。 evaluating_method=’anonymeter-inference’ Inference risk represents the possibility that, even after Privacy-Enhancing Techniques have been applied, there is still a significant chance of deducing the value of an attribute from the values of a set of other attributes. In the example from the Anonymeter, it refers to the scenario where “a person with attributes X and Y also have Z”. To phrase it differently, even if attackers cannot single out individual identities or cannot link different records, they may still be able to deduce specific information via statistical analysis or other methods. 推斷性風險代表的是即使經過隱私強化技術處理，仍有多大的可能，從一組其他的特徵中推斷出某個特徵的值。以 Anonymeter 的舉例，就是「擁有特徵 X 和特徵 Y 的人也擁有特徵 Z」。也就是說，即使攻擊者無法指認個體身分、也無法連結不同紀錄，攻擊者仍可以透過統計分析或其他方法來推斷出特定的資訊。 secret and aux_cols secret (str) Column(s) of secret information 秘密資訊欄位 aux_cols (List[str]) Columns of auxiliary information 輔助資訊欄位 In the context of Inference risk, the parameters secret and aux_cols go hand in hand. secret represents the attribute that is kept confidential, and in this scenario, aux_cols are the attributes other than secret that are considered to provide auxiliary information to the attacker. The example provided by Anonymeter suggests the following configuration: 在推斷性風險中，secret 與 aux_cols 參數是一體兩面的，secret 代表被保密的屬性 (attribute)，此時 aux_cols 則是除了 secret 以外的屬性、都被認為可以提供攻擊者輔助資訊。 Anonymeter 的範例建議了以下的設定方法： columns = ori.columns for secret in columns: aux_cols = [col for col in columns if col != secret] evaluator = InferenceEvaluator( aux_cols=aux_cols, secret=secret, ... ) This approach allows us to iterate through each column considered as a secret. And following the method outlined in the Anonymeter paper, averaging all the risk results for the secret attributes results in the dataset’s overall inference risk (not detailed in the paper, but calculated as the arithmetic mean by PETsARD). Currently, both secret and aux_cols have no default values, and it is recommended for users to manually set aux_cols as all attributes except for the secret. In future updates, following the experimental approach outlined in the Anonymeter paper, the attacker’s auxiliary information will be considered in a range from “only one column other than secret” to “all columns other than secret,” and these options will be provided as default values. 這樣能遍歷每個欄位被視作 secret。然後參考 Anonymeter 論文的方法，對所有 secret 的風險結果取平均、則為資料集整體的 推論性風險（論文中沒詳細描述，但 PETsARD 使用算術平均）。 目前 secret 跟 aux_cols 都沒有預設值，建議使用者手動設定為 aux_cols 統一為 secret 以外的所有屬性。在未來更新中，將依照 Anonymeter 論文的實驗方式，將攻擊者的輔助資訊從「除 secret 以外僅有一列」到「除 secret 以外所有列」所有抽樣方式都遍歷考慮一次，來提供這樣的預設值。 Refenece For explanations of the library in this paper and translations of terminologies between Chinese and English, please refer to the following references: 本文之函式庫解釋與中英用詞翻譯，請參閱以下文獻： Giomi, M., Boenisch, F., Wehmeyer, C., &amp; Tasnádi, B. (2023). A Unified Framework for Quantifying Privacy Risk in Synthetic Data. Proceedings of Privacy Enhancing Technologies Symposium, 2023(2), 312–328. https://doi.org/10.56553/popets-2023-0055 蔡柏毅（2021）。淺談個資「去識別化」與「合理利用」間的平衡。《金融聯合徵信》，第三十九期，2021年12月。"
  }}
