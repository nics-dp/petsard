name: üß™ Test Suite

on:
  push:
    branches: [main, dev]
    paths:
      - "petsard/**"
      - "tests/**"
      - "pyproject.toml"
      - ".github/workflows/test-suite.yml"
  pull_request:
    branches: [main, dev]
    paths:
      - "petsard/**"
      - "tests/**"
      - "pyproject.toml"
      - ".github/workflows/test-suite.yml"

permissions:
  contents: read
  pull-requests: write

jobs:
  test:
    name: üß™ Python ${{ matrix.python-version }}
    runs-on: ubuntu-latest
    continue-on-error: true

    strategy:
      matrix:
        python-version: ["3.10", "3.11"]

    steps:
      - name: üì• Checkout
        uses: actions/checkout@v5

      - name: üêç Setup Python ${{ matrix.python-version }}
        uses: actions/setup-python@v6
        with:
          python-version: ${{ matrix.python-version }}

      - name: üì¶ Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ matrix.python-version }}-${{ hashFiles('pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-${{ matrix.python-version }}-

      - name: üîß Install
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"

      - name: üîç Ruff Check
        id: ruff
        continue-on-error: true
        run: |
          ruff check . --output-format=json > ruff.json 2>&1 || true
          echo "status=$([ -s ruff.json ] && echo 'failed' || echo 'success')" >> $GITHUB_OUTPUT

      - name: üß™ Run Tests
        id: pytest
        continue-on-error: true
        run: |
          pytest tests/ \
            --cov=petsard \
            --cov-report=term \
            --cov-report=xml \
            --cov-report=json \
            --tb=short \
            --junit-xml=junit.xml \
            -v 2>&1 | tee pytest.log || true

          # Save exit code
          echo "status=$?" >> $GITHUB_OUTPUT

      - name: üìä Generate Report
        if: always()
        run: |
          python << 'PYTHON_SCRIPT'
          import json
          import xml.etree.ElementTree as ET
          from pathlib import Path

          report = []
          report.append("## üß™ Test Report\n")

          # === Ruff Results ===
          ruff_file = Path("ruff.json")
          if ruff_file.exists() and ruff_file.stat().st_size > 0:
              try:
                  with open("ruff.json") as f:
                      ruff_data = json.load(f)

                  if ruff_data:
                      report.append(f"### ‚ö†Ô∏è Code Quality ({len(ruff_data)} issues)\n")

                      # Group by file
                      by_file = {}
                      for issue in ruff_data[:20]:  # Show max 20
                          filename = issue.get('filename', 'unknown')
                          if filename not in by_file:
                              by_file[filename] = []
                          by_file[filename].append(issue)

                      for filename, issues in by_file.items():
                          report.append(f"\n**{filename}**\n")
                          for issue in issues[:5]:  # Max 5 per file
                              line = issue.get('location', {}).get('row', '?')
                              code = issue.get('code', '?')
                              msg = issue.get('message', 'No message')
                              report.append(f"- Line {line}: `{code}` {msg}\n")

                      if len(ruff_data) > 20:
                          report.append(f"\n*...{len(ruff_data) - 20} more issues*\n")
                      else:
                          report.append("### ‚úÖ Code Quality Passed\n")
                  except Exception as e:
                      report.append(f"### ‚ö†Ô∏è Unable to parse Ruff results: {e}\n")
              else:
                  report.append("### ‚úÖ Code Quality Passed\n")

          report.append("\n---\n\n")

          # === Pytest Results ===
          junit_file = Path("junit.xml")
          coverage_file = Path("coverage.json")

          if junit_file.exists():
              try:
                  tree = ET.parse("junit.xml")
                  root = tree.getroot()

                  total = int(root.get('tests', 0))
                  failures = int(root.get('failures', 0))
                  errors = int(root.get('errors', 0))
                  skipped = int(root.get('skipped', 0))
                  passed = total - failures - errors - skipped

                  # Title
                  if failures + errors == 0:
                      report.append(f"### ‚úÖ Tests Passed ({passed}/{total})\n\n")
                  else:
                      report.append(f"### ‚ùå Tests Failed ({passed}/{total} passed)\n\n")

                  # Coverage
                  if coverage_file.exists():
                      with open("coverage.json") as f:
                          cov_data = json.load(f)
                      coverage = cov_data.get('totals', {}).get('percent_covered', 0)
                      report.append(f"**Coverage**: {coverage:.1f}%\n\n")

                  # Failed tests
                  if failures + errors > 0:
                      report.append(f"**Failed Tests**:\n\n")
                      for testcase in root.iter('testcase'):
                          failure = testcase.find('failure')
                          error = testcase.find('error')
                          if failure is not None or error is not None:
                              classname = testcase.get('classname', '')
                              name = testcase.get('name', '')
                              report.append(f"- `{classname}::{name}`\n")

                              # Error message
                              msg = (failure or error).get('message', '')
                              if msg:
                                  # Only take first line
                                  first_line = msg.split('\n')[0][:100]
                                  report.append(f"  *{first_line}*\n")
                      report.append("\n")

                  # Skipped tests
                  if skipped > 0:
                      report.append(f"**Skipped**: {skipped} tests\n\n")

              except Exception as e:
                  report.append(f"### ‚ö†Ô∏è Unable to parse test results: {e}\n\n")
          else:
              report.append("### ‚ùå Test results not found\n\n")

          # === Detailed Output ===
          pytest_log = Path("pytest.log")
          if pytest_log.exists():
              report.append("\n<details>\n<summary>üìù Detailed Test Output</summary>\n\n```\n")
              with open("pytest.log") as f:
                  lines = f.readlines()
                  # Show only last 200 lines
                  report.extend(lines[-200:])
              report.append("\n```\n</details>\n")

          # Write report
          with open("test_report.md", "w") as f:
              f.writelines(report)

          print("".join(report))
          PYTHON_SCRIPT

      - name: üí¨ Comment on PR
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v8
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('test_report.md', 'utf8');

            // Find existing comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(c =>
              c.user.type === 'Bot' && c.body.includes('üß™ Test Report')
            );

            const body = `## üß™ Test Report (Python ${{ matrix.python-version }})\n\n${report}\n\n---\n*Updated at ${new Date().toISOString()}*`;

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body
              });
            }

      - name: üì§ Upload Artifacts
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: test-results-py${{ matrix.python-version }}
          path: |
            pytest.log
            junit.xml
            coverage.xml
            coverage.json
            ruff.json
            test_report.md
          retention-days: 7

      - name: ‚ÑπÔ∏è Summary
        if: always()
        run: |
          RUFF_STATUS="${{ steps.ruff.outputs.status }}"
          TEST_STATUS="${{ steps.pytest.outputs.status }}"

          if [[ "$RUFF_STATUS" == "success" && "$TEST_STATUS" == "0" ]]; then
            echo "::notice::‚úÖ Python ${{ matrix.python-version }}: All checks passed"
          else
            echo "::warning::‚ö†Ô∏è Python ${{ matrix.python-version }}: Some checks failed (does not block merge)"
          fi
