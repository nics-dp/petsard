"""
SDV Custom Methods Demo - SDV è‡ªè¨‚æ–¹æ³•ç¤ºç¯„
Demonstrates how to wrap SDV methods as PETsARD custom_method
ç¤ºç¯„å¦‚ä½•å°‡ SDV æ–¹æ³•å°è£ç‚º PETsARD custom_method

This script provides wrapper classes for SDV synthesizers that can be used
as custom methods in PETsARD, allowing users to leverage SDV's powerful
synthetic data generation capabilities without depending on SDV as a core dependency.
æœ¬è…³æœ¬æä¾› SDV åˆæˆå™¨çš„å°è£é¡åˆ¥ï¼Œå¯ä½œç‚º PETsARD çš„è‡ªè¨‚æ–¹æ³•ä½¿ç”¨ï¼Œ
è®“ä½¿ç”¨è€…èƒ½å¤ åˆ©ç”¨ SDV å¼·å¤§çš„åˆæˆè³‡æ–™ç”Ÿæˆèƒ½åŠ›ï¼Œè€Œç„¡éœ€å°‡ SDV ä½œç‚ºæ ¸å¿ƒä¾è³´ã€‚

Requirements éœ€æ±‚:
    pip install sdv

Supported Methods æ”¯æ´çš„æ–¹æ³•:
    - SDV_GaussianCopula: Fast statistical method å¿«é€Ÿã€åŸºæ–¼çµ±è¨ˆçš„æ–¹æ³•
    - SDV_CTGAN: High-quality GAN method é«˜å“è³ª GAN æ–¹æ³•
    - SDV_CopulaGAN: Hybrid statistical + GAN method æ··åˆçµ±è¨ˆå’Œ GAN çš„æ–¹æ³•
    - SDV_TVAE: VAE-based method åŸºæ–¼ VAE çš„æ–¹æ³•
"""

import logging
import time
import warnings
from typing import Any

import pandas as pd

from petsard.metadater import Schema


def check_sdv_installed():
    """
    Check if SDV is installed æª¢æŸ¥ SDV æ˜¯å¦å·²å®‰è£

    Raises:
        ImportError: If SDV is not installed å¦‚æœ SDV æœªå®‰è£
    """
    try:
        import sdv

        return True
    except ImportError:
        error_msg = """
        SDV package is not installed! SDV å¥—ä»¶æœªå®‰è£ï¼

        Please install SDV using: è«‹ä½¿ç”¨ä»¥ä¸‹æŒ‡ä»¤å®‰è£ SDVï¼š
            pip install sdv

        Or install a specific version: æˆ–å®‰è£ç‰¹å®šç‰ˆæœ¬ï¼š
            pip install sdv==1.15.0

        For more information: æ›´å¤šè³‡è¨Šè«‹åƒè€ƒï¼šhttps://sdv.dev/
        """
        raise ImportError(error_msg)


def schema_to_sdv(schema: Schema, data: pd.DataFrame = None) -> dict[str, Any]:
    """
    Convert PETsARD Schema to SDV metadata format å°‡ PETsARD Schema è½‰æ›ç‚º SDV metadata æ ¼å¼

    Args:
        schema: PETsARD Schema object PETsARD Schema ç‰©ä»¶
        data: Optional training data to extract category values å¯é¸çš„è¨“ç·´æ•¸æ“šä»¥æå–é¡åˆ¥å€¼

    Returns:
        dict: SDV metadata dictionary SDV metadata æ ¼å¼çš„å­—å…¸
    """
    # ğŸ” DIAGNOSTIC: Track conversion process
    print("\nğŸ” DIAGNOSTIC: schema_to_sdv() called")
    print(f"  Schema has {len(schema.attributes)} attributes")
    if data is not None:
        print(f"  Data provided: shape {data.shape}")

    sdv_metadata = {"columns": {}, "METADATA_SPEC_VERSION": "SINGLE_TABLE_V1"}

    for attr_name, attribute in schema.attributes.items():
        sdtype = _map_attribute_to_sdv_type(attribute)
        col_info = {"sdtype": sdtype}

        # If data is provided and column is categorical, extract values
        # å¦‚æœæä¾›äº†æ•¸æ“šä¸”æ¬„ä½æ˜¯é¡åˆ¥å‹ï¼Œæå–é¡åˆ¥å€¼
        if data is not None and attr_name in data.columns and sdtype == "categorical":
            unique_vals = data[attr_name].dropna().unique()
            # Only include values list for reasonable cardinality (< 100)
            # åªå°åˆç†åŸºæ•¸ï¼ˆ< 100ï¼‰çš„æ¬„ä½åŒ…å«å€¼åˆ—è¡¨
            if len(unique_vals) < 100:
                col_info["values"] = [str(v) for v in unique_vals]
                print(
                    f"  âœ“ Extracted {len(unique_vals)} unique values for '{attr_name}'"
                )
            else:
                print(
                    f"  âš  Skipped values for '{attr_name}' (too many: {len(unique_vals)})"
                )

        sdv_metadata["columns"][attr_name] = col_info

    # ğŸ” DIAGNOSTIC: Show complete SDV metadata structure
    print(f"  Converted to SDV metadata with {len(sdv_metadata['columns'])} columns")
    # print("\n  ğŸ“‹ å®Œæ•´çš„ SDV Metadata Dictionary:")
    # print("  " + "=" * 70)
    # import json
    # print("  " + json.dumps(sdv_metadata, indent=4).replace("\n", "\n  "))
    # print("  " + "=" * 70)

    return sdv_metadata


def _map_attribute_to_sdv_type(attribute: Any) -> str:
    """
    Map PETsARD Attribute to SDV sdtype å°‡ PETsARD Attribute å°æ‡‰åˆ° SDV sdtype

    Args:
        attribute: PETsARD Attribute object or dict PETsARD Attribute ç‰©ä»¶æˆ– dict

    Returns:
        str: SDV sdtype
    """
    # Handle both dict and Attribute object è™•ç† dict å’Œ Attribute ç‰©ä»¶å…©ç¨®æƒ…æ³
    if isinstance(attribute, dict):
        category = attribute.get("category")
        logical_type = attribute.get("logical_type")
        attr_type = attribute.get("type")
    else:
        category = attribute.category
        logical_type = attribute.logical_type
        attr_type = attribute.type

    # PRIORITY 1: Check data type first for numerical types
    # å„ªå…ˆç´š 1ï¼šå°æ–¼ numerical é¡å‹ï¼Œå„ªå…ˆæª¢æŸ¥è³‡æ–™é¡å‹
    # This prevents integer/float from being incorrectly marked as categorical
    # é€™é˜²æ­¢ integer/float è¢«éŒ¯èª¤åœ°æ¨™è¨˜ç‚º categorical
    if attr_type:
        attr_type_lower = attr_type.lower()
        if "int" in attr_type_lower or "float" in attr_type_lower:
            # For numerical types, only treat as categorical if category is explicitly True
            # AND it's a reasonable categorical variable (not high cardinality)
            # å°æ–¼ numerical é¡å‹ï¼Œåªæœ‰ç•¶ category æ˜ç¢ºç‚º True æ™‚æ‰è¦–ç‚º categorical
            # æ³¨æ„ï¼šé€™è£¡æˆ‘å€‘ä¿¡ä»» schema çš„æ¨™è¨˜ï¼Œä½† SDV æœƒæ ¹æ“šå¯¦éš›è³‡æ–™åšæœ€çµ‚åˆ¤æ–·
            if category is True:
                # Integer/float marked as category: let SDV decide based on cardinality
                # æ¨™è¨˜ç‚º category çš„ integer/floatï¼šè®“ SDV æ ¹æ“šåŸºæ•¸æ±ºå®š
                # For now, we'll trust the numerical type over the category flag
                # ç›®å‰ï¼Œæˆ‘å€‘ä¿¡ä»» numerical é¡å‹è€Œé category æ¨™è¨˜
                return "numerical"
            return "numerical"
        elif "bool" in attr_type_lower:
            return "boolean"
        elif "datetime" in attr_type_lower:
            return "datetime"

    # PRIORITY 2: Check logical_type for semantic types
    # å„ªå…ˆç´š 2ï¼šæª¢æŸ¥ logical_type çš„èªç¾©é¡å‹ï¼ˆemailã€phone ç­‰ï¼‰
    if logical_type:
        logical = logical_type.lower()
        if logical in ["email", "phone", "ssn", "credit_card"]:
            return "pii"
        elif logical in ["datetime", "date", "time", "timestamp"]:
            return "datetime"
        elif logical in ["address", "ip", "url"]:
            return "pii"

    # PRIORITY 3: Check category field for string types
    # å„ªå…ˆç´š 3ï¼šå°æ–¼å­—ä¸²é¡å‹æª¢æŸ¥ category æ¬„ä½
    if category is True:
        return "categorical"

    # PRIORITY 4: Default based on remaining type
    # å„ªå…ˆç´š 4ï¼šæ ¹æ“šå‰©é¤˜é¡å‹è¨­å®šé è¨­å€¼
    if attr_type:
        attr_type_lower = attr_type.lower()
        if attr_type_lower in ["string", "str", "object"]:
            # String types default to categorical
            # å­—ä¸²é¡å‹é è¨­ç‚º categorical
            return "categorical"

    # Default to categorical for unknown types
    # æœªçŸ¥é¡å‹é è¨­ç‚º categorical
    return "categorical"


class BaseSDVSynthesizer:
    """
    Base class for SDV Synthesizers SDV Synthesizer åŸºç¤é¡åˆ¥

    Base wrapper class for all SDV methods, handling common initialization
    and metadata conversion logic
    æ‰€æœ‰ SDV æ–¹æ³•çš„åŸºç¤å°è£é¡ï¼Œè™•ç†å…±åŒçš„åˆå§‹åŒ–å’Œå…ƒæ•¸æ“šè½‰æ›é‚è¼¯
    """

    def __init__(self, config: dict, metadata: Schema = None):
        """
        Initialize SDV synthesizer åˆå§‹åŒ– SDV synthesizer

        Args:
            config: Configuration dictionary with sample_num_rows etc. é…ç½®å­—å…¸ï¼ŒåŒ…å« sample_num_rows ç­‰åƒæ•¸
            metadata: PETsARD Schema metadata (optional) PETsARD Schema å…ƒæ•¸æ“šï¼ˆå¯é¸ï¼‰
        """
        init_start = time.time()

        # Check if SDV is installed æª¢æŸ¥ SDV æ˜¯å¦å®‰è£
        step_start = time.time()
        check_sdv_installed()

        from sdv.metadata import Metadata as SDV_Metadata

        self.config = config
        self.metadata = metadata
        self.logger = logging.getLogger(f"PETsARD.{self.__class__.__name__}")
        self.logger.info(f"â±ï¸  SDV import: {time.time() - step_start:.3f}s")

        # ğŸ” DIAGNOSTIC: Check if metadata is provided
        print("=" * 70)
        print("ğŸ” DIAGNOSTIC: Checking metadata in __init__")
        print("=" * 70)
        if metadata is None:
            print("âš ï¸  WARNING: metadata is None! SDV will infer types from data later.")
        else:
            print(f"âœ“ Received metadata object: {type(metadata)}")
            if hasattr(metadata, "attributes"):
                print(f"âœ“ Metadata has {len(metadata.attributes)} attributes:")
                for attr_name, attr in list(metadata.attributes.items())[:5]:
                    print(
                        f"  - {attr_name}: type={getattr(attr, 'type', 'N/A')}, logical_type={getattr(attr, 'logical_type', 'N/A')}"
                    )
                if len(metadata.attributes) > 5:
                    print(f"  ... and {len(metadata.attributes) - 5} more attributes")
        print("=" * 70)

        self._synthesizer = None
        self._sdv_metadata = None
        self._original_dtypes = {}  # Record original dtypes for restoration
        self._original_pandas_dtypes = {}  # Record actual pandas dtypes from data

        # Handle device configuration è™•ç†è¨­å‚™é…ç½®
        step_start = time.time()
        self._setup_device()
        self.logger.info(f"â±ï¸  Device setup: {time.time() - step_start:.3f}s")

        # If metadata is provided, convert to SDV format å¦‚æœæä¾›äº† metadataï¼Œé å…ˆè½‰æ›ç‚º SDV æ ¼å¼
        if metadata is not None:
            step_start = time.time()
            self.logger.info(
                f"Converting PETsARD Schema to SDV metadata (columns: {len(metadata.attributes)})"
            )
            print("ğŸ” DIAGNOSTIC: Converting metadata to SDV format...")
            sdv_metadata_dict = schema_to_sdv(metadata)
            self.logger.info(f"â±ï¸  Schema conversion: {time.time() - step_start:.3f}s")

            step_start = time.time()
            with warnings.catch_warnings(record=True) as w:
                warnings.filterwarnings(
                    "always", message="No table name was provided.*"
                )
                self._sdv_metadata = SDV_Metadata.load_from_dict(sdv_metadata_dict)
                for warning in w:
                    self.logger.debug(f"SDV Metadata: {warning.message}")
            print("âœ“ Successfully created SDV_Metadata object")
            print(f"  SDV_Metadata type: {type(self._sdv_metadata)}")
            self.logger.info(
                f"â±ï¸  SDV Metadata.load_from_dict: {time.time() - step_start:.3f}s"
            )

        self.logger.info(f"âœ… Total __init__ time: {time.time() - init_start:.3f}s")

    def _setup_device(self):
        """
        Setup compute device (CPU/CUDA) è¨­å®šè¨ˆç®—è¨­å‚™ï¼ˆCPU/CUDAï¼‰

        Raises:
            RuntimeError: If CUDA is requested but not available å¦‚æœè¦æ±‚ CUDA ä½†ä¸å¯ç”¨
        """
        import torch

        # Get device setting from config (default: 'cpu') å¾é…ç½®å–å¾—è¨­å‚™è¨­å®šï¼ˆé è¨­ï¼š'cpu'ï¼‰
        device_config = self.config.get("device", "cpu").lower()

        if device_config == "cuda":
            # User explicitly requested CUDA ç”¨æˆ¶æ˜ç¢ºè¦æ±‚ CUDA
            if not torch.cuda.is_available():
                error_msg = (
                    "CUDA device requested but CUDA is not available! "
                    "CUDA è¨­å‚™è¢«è¦æ±‚ä½† CUDA ä¸å¯ç”¨ï¼\n"
                    "Please check: è«‹æª¢æŸ¥ï¼š\n"
                    "1. PyTorch is installed with CUDA support PyTorch æ˜¯å¦å®‰è£äº† CUDA æ”¯æ´\n"
                    "2. NVIDIA GPU is available NVIDIA GPU æ˜¯å¦å¯ç”¨\n"
                    "3. CUDA drivers are properly installed CUDA é©…å‹•æ˜¯å¦æ­£ç¢ºå®‰è£"
                )
                self.logger.error(error_msg)
                raise RuntimeError(error_msg)

            self.use_cuda = True
            self.logger.info("Using CUDA device for training ä½¿ç”¨ CUDA è¨­å‚™é€²è¡Œè¨“ç·´")

        elif device_config == "cpu":
            # User requested CPU or default CPU ç”¨æˆ¶è¦æ±‚ CPU æˆ–é è¨­ CPU
            self.use_cuda = False
            self.logger.info("Using CPU device for training ä½¿ç”¨ CPU è¨­å‚™é€²è¡Œè¨“ç·´")

        else:
            # Invalid device specification ç„¡æ•ˆçš„è¨­å‚™è¦æ ¼
            error_msg = (
                f"Invalid device '{device_config}'. Must be 'cpu' or 'cuda'. "
                f"ç„¡æ•ˆçš„è¨­å‚™ '{device_config}'ã€‚å¿…é ˆæ˜¯ 'cpu' æˆ– 'cuda'ã€‚"
            )
            self.logger.error(error_msg)
            raise ValueError(error_msg)

    def _create_sdv_metadata(self, data: pd.DataFrame):
        """
        Create SDV metadata from data (used when metadata not provided)
        å¾æ•¸æ“šå‰µå»º SDV metadataï¼ˆç•¶æœªæä¾› metadata æ™‚ä½¿ç”¨ï¼‰

        Args:
            data: Training data è¨“ç·´æ•¸æ“š
        """
        from sdv.metadata import Metadata as SDV_Metadata

        from petsard.metadater.metadater import SchemaMetadater

        print("\nğŸ” DIAGNOSTIC: _create_sdv_metadata() called - FALLBACK MODE!")
        print(f"  Data shape: {data.shape}")
        print(f"  Data dtypes sample: {dict(list(data.dtypes.items())[:3])}")

        step_start = time.time()
        self.logger.info(f"Creating schema from data (shape: {data.shape})...")
        schema = SchemaMetadater.from_data(data)
        self.logger.info(
            f"  â±ï¸  SchemaMetadater.from_data: {time.time() - step_start:.3f}s"
        )

        step_start = time.time()
        print("  Converting inferred schema to SDV format with data...")
        sdv_metadata_dict = schema_to_sdv(schema, data=data)
        self.logger.info(f"  â±ï¸  schema_to_sdv: {time.time() - step_start:.3f}s")

        step_start = time.time()
        with warnings.catch_warnings(record=True) as w:
            warnings.filterwarnings("always", message="No table name was provided.*")
            self._sdv_metadata = SDV_Metadata.load_from_dict(sdv_metadata_dict)
            for warning in w:
                self.logger.debug(f"SDV Metadata: {warning.message}")
        self.logger.info(
            f"  â±ï¸  SDV Metadata.load_from_dict: {time.time() - step_start:.3f}s"
        )

    def _initialize_synthesizer(self):
        """
        Initialize SDV synthesizer instance (implemented by subclass)
        åˆå§‹åŒ– SDV synthesizer å¯¦ä¾‹ï¼ˆç”±å­é¡å¯¦ç¾ï¼‰

        Raises:
            NotImplementedError: Must be implemented by subclass å¿…é ˆç”±å­é¡å¯¦ç¾
        """
        raise NotImplementedError("Subclass must implement _initialize_synthesizer")

    def fit(self, data: pd.DataFrame) -> None:
        """
        Train the synthesizer è¨“ç·´ synthesizer

        Args:
            data: Training data è¨“ç·´æ•¸æ“š
        """
        fit_start = time.time()
        self.logger.info(f"ğŸš€ Starting fit() for {self.__class__.__name__}")

        # Record original dtypes from metadata if available, otherwise from data
        # This is crucial because data may have been preprocessed (e.g., Int64 -> float64)
        # å¾ metadata è¨˜éŒ„åŸå§‹ dtypesï¼ˆå¦‚æœå¯ç”¨ï¼‰ï¼Œå¦å‰‡å¾æ•¸æ“šè¨˜éŒ„

        # TIMING: Metadata check
        step_start = time.time()
        self.logger.debug(f"[DEBUG] metadata is None: {self.metadata is None}")
        if self.metadata is not None:
            self.logger.debug(f"[DEBUG] metadata type: {type(self.metadata)}")
            self.logger.debug(
                f"[DEBUG] metadata has attributes: {hasattr(self.metadata, 'attributes')}"
            )
            if hasattr(self.metadata, "attributes"):
                self.logger.debug(
                    f"[DEBUG] attributes count: {len(self.metadata.attributes)}"
                )
        self.logger.info(f"â±ï¸  Metadata check: {time.time() - step_start:.3f}s")

        # TIMING: Extract original dtypes
        step_start = time.time()
        # é€™å¾ˆé‡è¦ï¼Œå› ç‚ºè³‡æ–™å¯èƒ½å·²è¢«é è™•ç†ï¼ˆä¾‹å¦‚ Int64 -> float64ï¼‰
        if self.metadata is not None and hasattr(self.metadata, "attributes"):
            # Extract original dtypes from PETsARD Schema
            # å¾ PETsARD Schema æå–åŸå§‹ dtypes
            self._original_dtypes = {}
            for attr_name, attribute in self.metadata.attributes.items():
                if attr_name in data.columns:
                    # Use the type from metadata, which preserves original dtype
                    # ä½¿ç”¨ metadata ä¸­çš„é¡å‹ï¼Œä¿ç•™åŸå§‹ dtype
                    original_type = attribute.type
                    self._original_dtypes[attr_name] = original_type
            self.logger.info(
                f"âœ“ Recorded {len(self._original_dtypes)} dtypes from metadata"
            )
        else:
            # Fallback: record dtypes from training data
            # å›é€€æ–¹æ¡ˆï¼šå¾è¨“ç·´æ•¸æ“šè¨˜éŒ„ dtypes
            self._original_dtypes = {col: data[col].dtype for col in data.columns}
            self.logger.warning(
                f"âœ— Fallback - Recorded {len(self._original_dtypes)} dtypes from data"
            )
        self.logger.info(f"â±ï¸  Extract dtypes: {time.time() - step_start:.3f}s")

        # TIMING: Create SDV metadata
        step_start = time.time()
        # If no metadata, create from data å¦‚æœæ²’æœ‰ metadataï¼Œå¾æ•¸æ“šå‰µå»º
        print("\nğŸ” DIAGNOSTIC: Checking _sdv_metadata in fit()")
        if self._sdv_metadata is None:
            print("âš ï¸  _sdv_metadata is None! Will create from data...")
            print("âš ï¸  This means SDV will INFER types instead of using your schema!")
            self.logger.warning(
                "âš ï¸  _sdv_metadata is None! Creating from data - SDV will infer types!"
            )
            self._create_sdv_metadata(data)
        else:
            print("âœ“ _sdv_metadata exists, using pre-configured schema")
            print(f"  SDV_Metadata object: {type(self._sdv_metadata)}")
        self.logger.info(f"â±ï¸  Create SDV metadata: {time.time() - step_start:.3f}s")

        # TIMING: Initialize synthesizer
        step_start = time.time()
        # Initialize synthesizer åˆå§‹åŒ– synthesizer
        if self._synthesizer is None:
            self.logger.info("Initializing synthesizer...")
            self._initialize_synthesizer()
        self.logger.info(f"â±ï¸  Initialize synthesizer: {time.time() - step_start:.3f}s")

        # TIMING: Train
        step_start = time.time()
        # Train è¨“ç·´
        self.logger.info(
            f"Training {self.__class__.__name__} with data shape: {data.shape}"
        )

        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter("always")
            self._synthesizer.fit(data)
            for warning in w:
                self.logger.debug(f"Warning during fit: {warning.message}")

        self.logger.info(f"â±ï¸  SDV fit() execution: {time.time() - step_start:.3f}s")
        self.logger.info(f"âœ… Total fit() time: {time.time() - fit_start:.3f}s")

    def sample(self) -> pd.DataFrame:
        """
        Generate synthetic data ç”Ÿæˆåˆæˆæ•¸æ“š

        Returns:
            pd.DataFrame: Synthetic data åˆæˆæ•¸æ“š
        """
        if self._synthesizer is None:
            raise RuntimeError("Synthesizer has not been fitted yet. Call fit() first.")

        num_rows = self.config.get("sample_num_rows", 100)
        batch_size = self.config.get("batch_size")

        self.logger.info(f"Sampling {num_rows} rows")

        synthetic_data = self._synthesizer.sample(
            num_rows=num_rows, batch_size=batch_size
        )

        # Restore original dtypes (especially important for Int64, Int32, etc.)
        # æ¢å¾©åŸå§‹ dtypesï¼ˆå° Int64, Int32 ç­‰ç‰¹åˆ¥é‡è¦ï¼‰
        synthetic_data = self._restore_dtypes(synthetic_data)

        self.logger.info(f"Successfully generated {len(synthetic_data)} rows")
        return synthetic_data

    def _restore_dtypes(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        Restore original dtypes to synthetic data
        æ¢å¾©åˆæˆæ•¸æ“šçš„åŸå§‹ dtype

        Args:
            data: Synthetic data åˆæˆæ•¸æ“š

        Returns:
            pd.DataFrame: Data with restored dtypes æ¢å¾© dtype å¾Œçš„æ•¸æ“š
        """
        for col in data.columns:
            if col not in self._original_dtypes:
                continue

            original_dtype = self._original_dtypes[col]
            # Handle both pandas dtype objects and string representations
            # è™•ç† pandas dtype ç‰©ä»¶å’Œå­—ä¸²è¡¨ç¤º
            dtype_str = str(original_dtype)

            try:
                # Handle schema 'integer' type - map to Int64 (pandas nullable integer)
                # è™•ç† schema çš„ 'integer' é¡å‹ - æ˜ å°„åˆ° Int64ï¼ˆpandas nullable integerï¼‰
                if dtype_str == "integer":
                    data[col] = data[col].round().astype("Int64")
                    self.logger.info(
                        f"âœ“ Restored {col} from schema type 'integer' to 'Int64'"
                    )
                # Handle Pandas nullable integer types è™•ç† Pandas nullable integer é¡å‹
                elif dtype_str in [
                    "Int8",
                    "Int16",
                    "Int32",
                    "Int64",
                    "UInt8",
                    "UInt16",
                    "UInt32",
                    "UInt64",
                ]:
                    # Round to integer and convert to nullable integer type
                    # å››æ¨äº”å…¥ç‚ºæ•´æ•¸ä¸¦è½‰æ›ç‚º nullable integer é¡å‹
                    data[col] = data[col].round().astype(dtype_str)
                    self.logger.debug(
                        f"Restored {col} to nullable integer type {dtype_str}"
                    )
                # Handle standard integer types è™•ç†æ¨™æº–æ•´æ•¸é¡å‹
                elif (
                    any(
                        int_type in dtype_str.lower()
                        for int_type in [
                            "int8",
                            "int16",
                            "int32",
                            "int64",
                            "uint8",
                            "uint16",
                            "uint32",
                            "uint64",
                        ]
                    )
                    and "Int" not in dtype_str
                ):
                    # Standard numpy integer types
                    # æ¨™æº– numpy æ•´æ•¸é¡å‹
                    data[col] = data[col].round().astype(dtype_str)
                    self.logger.debug(
                        f"Restored {col} to standard integer type {dtype_str}"
                    )
                # Handle datetime types è™•ç†æ—¥æœŸæ™‚é–“é¡å‹
                elif "datetime" in dtype_str.lower():
                    if pd.api.types.is_numeric_dtype(data[col]):
                        data[col] = pd.to_datetime(
                            data[col].round().astype("int64"), unit="ns"
                        )
                    else:
                        data[col] = pd.to_datetime(data[col])
                    self.logger.debug(f"Restored {col} to datetime type")
                # Handle timedelta types è™•ç†æ™‚é–“å·®é¡å‹
                elif "timedelta" in dtype_str.lower():
                    if pd.api.types.is_numeric_dtype(data[col]):
                        data[col] = pd.to_timedelta(
                            data[col].round().astype("int64"), unit="ns"
                        )
                    else:
                        data[col] = pd.to_timedelta(data[col])
                    self.logger.debug(f"Restored {col} to timedelta type")
                # Handle float types è™•ç†æµ®é»æ•¸é¡å‹
                elif any(
                    float_type in dtype_str.lower()
                    for float_type in ["float16", "float32", "float64"]
                ):
                    data[col] = data[col].astype(dtype_str)
                    self.logger.debug(f"Restored {col} to float type {dtype_str}")
                # Handle boolean types è™•ç†å¸ƒæ—é¡å‹
                elif "bool" in dtype_str.lower():
                    data[col] = data[col].astype(dtype_str)
                    self.logger.debug(f"Restored {col} to boolean type {dtype_str}")
                # Handle string/object types è™•ç†å­—ä¸²/ç‰©ä»¶é¡å‹
                elif dtype_str in ["object", "string", "str"]:
                    data[col] = data[col].astype("object")
                    self.logger.debug(f"Restored {col} to object type")
                # Try direct conversion for other types å…¶ä»–é¡å‹å˜—è©¦ç›´æ¥è½‰æ›
                else:
                    data[col] = data[col].astype(dtype_str)
                    self.logger.debug(f"Restored {col} to type {dtype_str}")
            except Exception as e:
                self.logger.warning(
                    f"Failed to restore dtype for {col} to {dtype_str}: {e}, keeping as is"
                )

        return data


class SDV_GaussianCopula(BaseSDVSynthesizer):
    """
    SDV GaussianCopula Synthesizer

    Fast statistical method suitable for åŸºæ–¼çµ±è¨ˆåˆ†ä½ˆçš„å¿«é€Ÿåˆæˆæ–¹æ³•ï¼Œé©åˆï¼š
    - Fast prototyping å¿«é€ŸåŸå‹æ¸¬è©¦
    - Large datasets å¤§å‹è³‡æ–™é›†
    - Numerical data æ•¸å€¼å‹è³‡æ–™

    Supported parameters æ”¯æ´åƒæ•¸ï¼š
        - default_distribution: Default distribution type é è¨­åˆ†ä½ˆé¡å‹ (truncnorm, beta, gamma, uniform, etc.)
        - numerical_distributions: Column-specific distributions ç‰¹å®šæ¬„ä½çš„åˆ†ä½ˆè¨­å®š
        - enforce_min_max_values: Enforce numerical range constraints å¼·åˆ¶æ•¸å€¼ç¯„åœé™åˆ¶
        - enforce_rounding: Force rounding for integer columns å¼·åˆ¶æ•´æ•¸æ¬„ä½å››æ¨äº”å…¥
    """

    def _initialize_synthesizer(self):
        """Initialize GaussianCopula synthesizer åˆå§‹åŒ– GaussianCopula synthesizer"""
        from sdv.single_table import GaussianCopulaSynthesizer

        init_params = {
            "metadata": self._sdv_metadata,
            "enforce_rounding": self.config.get("enforce_rounding", True),
            "enforce_min_max_values": self.config.get("enforce_min_max_values", True),
        }

        # Note: GaussianCopula doesn't use GPU, so cuda parameter is not applicable
        # æ³¨æ„ï¼šGaussianCopula ä¸ä½¿ç”¨ GPUï¼Œå› æ­¤ cuda åƒæ•¸ä¸é©ç”¨
        if self.use_cuda:
            self.logger.warning(
                "GaussianCopula does not support CUDA acceleration. Using CPU. "
                "GaussianCopula ä¸æ”¯æ´ CUDA åŠ é€Ÿã€‚ä½¿ç”¨ CPUã€‚"
            )

        # Add distribution-related parameters æ·»åŠ åˆ†ä½ˆç›¸é—œåƒæ•¸
        if "default_distribution" in self.config:
            init_params["default_distribution"] = self.config["default_distribution"]

        if "numerical_distributions" in self.config:
            init_params["numerical_distributions"] = self.config[
                "numerical_distributions"
            ]

        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter("always")
            self._synthesizer = GaussianCopulaSynthesizer(**init_params)
            for warning in w:
                self.logger.debug(f"Warning during initialization: {warning.message}")


class SDV_CTGAN(BaseSDVSynthesizer):
    """
    SDV CTGAN Synthesizer

    GAN-based deep learning method suitable for åŸºæ–¼ GAN çš„æ·±åº¦å­¸ç¿’æ–¹æ³•ï¼Œé©åˆï¼š
    - High-quality synthetic data éœ€è¦é«˜å“è³ªåˆæˆè³‡æ–™
    - Complex data patterns è¤‡é›œè³‡æ–™æ¨¡å¼
    - GPU-accelerated environments æœ‰ GPU åŠ é€Ÿçš„ç’°å¢ƒ

    Supported parameters æ”¯æ´åƒæ•¸ï¼š
        - epochs: Training epochs è¨“ç·´è¼ªæ•¸ (default é è¨­ 300)
        - batch_size: Batch size æ‰¹æ¬¡å¤§å° (default é è¨­ 500, must be divisible by pac å¿…é ˆèƒ½è¢« pac æ•´é™¤)
        - pac: PAC (Packing) size PAC æ‰“åŒ…å¤§å° (default é è¨­ 10, batch_size must be divisible by this)
        - discriminator_steps: Discriminator training steps åˆ¤åˆ¥å™¨è¨“ç·´æ­¥æ•¸ (default é è¨­ 1)
        - generator_lr: Generator learning rate ç”Ÿæˆå™¨å­¸ç¿’ç‡ (default é è¨­ 0.0002)
        - discriminator_lr: Discriminator learning rate åˆ¤åˆ¥å™¨å­¸ç¿’ç‡ (default é è¨­ 0.0002)
        - generator_dim: Generator dimensions ç”Ÿæˆå™¨ç¶­åº¦ (default é è¨­ (256, 256))
        - discriminator_dim: Discriminator dimensions åˆ¤åˆ¥å™¨ç¶­åº¦ (default é è¨­ (256, 256))
        - enforce_rounding: Force rounding for integer columns å¼·åˆ¶æ•´æ•¸æ¬„ä½å››æ¨äº”å…¥
    """

    def _initialize_synthesizer(self):
        """Initialize CTGAN synthesizer åˆå§‹åŒ– CTGAN synthesizer"""
        step_start = time.time()
        from sdv.single_table import CTGANSynthesizer

        self.logger.info(
            f"  â±ï¸  Import CTGANSynthesizer: {time.time() - step_start:.3f}s"
        )

        init_params = {
            "metadata": self._sdv_metadata,
            "enforce_rounding": self.config.get("enforce_rounding", True),
            "epochs": self.config.get("epochs", 300),
            "batch_size": self.config.get("batch_size", 500),
            "cuda": self.use_cuda,  # Enable/disable CUDA å•Ÿç”¨/ç¦ç”¨ CUDA
        }

        # Add optional parameters æ·»åŠ å¯é¸åƒæ•¸
        optional_params = [
            "pac",
            "discriminator_steps",
            "generator_lr",
            "discriminator_lr",
            "generator_dim",
            "discriminator_dim",
            "verbose",
        ]

        for param in optional_params:
            if param in self.config:
                init_params[param] = self.config[param]

        self.logger.info(
            f"Creating CTGANSynthesizer with params: epochs={init_params['epochs']}, batch_size={init_params['batch_size']}, cuda={init_params['cuda']}"
        )
        step_start = time.time()
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter("always")
            self._synthesizer = CTGANSynthesizer(**init_params)
            for warning in w:
                self.logger.debug(f"Warning during initialization: {warning.message}")
        self.logger.info(
            f"  â±ï¸  CTGANSynthesizer() instantiation: {time.time() - step_start:.3f}s"
        )


class SDV_CopulaGAN(BaseSDVSynthesizer):
    """
    SDV CopulaGAN Synthesizer

    Combines Copula statistics and GAN, suitable for çµåˆ Copula çµ±è¨ˆæ–¹æ³•å’Œ GANï¼Œé©åˆï¼š
    - Mixed data types (continuous + discrete) æ··åˆå‹è³‡æ–™ï¼ˆé€£çºŒ+é›¢æ•£ï¼‰
    - Better marginal distribution modeling éœ€è¦æ›´å¥½çš„é‚Šéš›åˆ†ä½ˆæ¨¡æ“¬
    - Balancing statistics and deep learning å¹³è¡¡çµ±è¨ˆå’Œæ·±åº¦å­¸ç¿’å„ªé»

    Supported parameters æ”¯æ´åƒæ•¸ï¼š
        - epochs: Training epochs è¨“ç·´è¼ªæ•¸ (default é è¨­ 300)
        - batch_size: Batch size æ‰¹æ¬¡å¤§å° (default é è¨­ 500, must be divisible by pac å¿…é ˆèƒ½è¢« pac æ•´é™¤)
        - pac: PAC (Packing) size PAC æ‰“åŒ…å¤§å° (default é è¨­ 10, batch_size must be divisible by this)
        - discriminator_steps: Discriminator training steps åˆ¤åˆ¥å™¨è¨“ç·´æ­¥æ•¸ (default é è¨­ 1)
        - generator_lr: Generator learning rate ç”Ÿæˆå™¨å­¸ç¿’ç‡ (default é è¨­ 0.0002)
        - discriminator_lr: Discriminator learning rate åˆ¤åˆ¥å™¨å­¸ç¿’ç‡ (default é è¨­ 0.0002)
        - default_distribution: Default distribution type é è¨­åˆ†ä½ˆé¡å‹ (default é è¨­ beta)
        - numerical_distributions: Column-specific distributions ç‰¹å®šæ¬„ä½çš„åˆ†ä½ˆè¨­å®š
        - enforce_rounding: Force rounding for integer columns å¼·åˆ¶æ•´æ•¸æ¬„ä½å››æ¨äº”å…¥
    """

    def _initialize_synthesizer(self):
        """Initialize CopulaGAN synthesizer åˆå§‹åŒ– CopulaGAN synthesizer"""
        from sdv.single_table import CopulaGANSynthesizer

        init_params = {
            "metadata": self._sdv_metadata,
            "enforce_rounding": self.config.get("enforce_rounding", True),
            "epochs": self.config.get("epochs", 300),
            "batch_size": self.config.get("batch_size", 500),
            "cuda": self.use_cuda,  # Enable/disable CUDA å•Ÿç”¨/ç¦ç”¨ CUDA
        }

        # Add distribution-related parameters æ·»åŠ åˆ†ä½ˆç›¸é—œåƒæ•¸
        if "default_distribution" in self.config:
            init_params["default_distribution"] = self.config["default_distribution"]

        if "numerical_distributions" in self.config:
            init_params["numerical_distributions"] = self.config[
                "numerical_distributions"
            ]

        # Add other optional parameters æ·»åŠ å…¶ä»–å¯é¸åƒæ•¸
        optional_params = [
            "pac",
            "discriminator_steps",
            "generator_lr",
            "discriminator_lr",
            "generator_dim",
            "discriminator_dim",
            "verbose",
        ]

        for param in optional_params:
            if param in self.config:
                init_params[param] = self.config[param]

        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter("always")
            self._synthesizer = CopulaGANSynthesizer(**init_params)
            for warning in w:
                self.logger.debug(f"Warning during initialization: {warning.message}")


class SDV_TVAE(BaseSDVSynthesizer):
    """
    SDV TVAE Synthesizer

    VAE-based generative model, suitable for åŸºæ–¼ VAE çš„ç”Ÿæˆæ¨¡å‹ï¼Œé©åˆï¼š
    - Stable training process éœ€è¦ç©©å®šè¨“ç·´éç¨‹
    - Medium-scale data ä¸­ç­‰è¦æ¨¡è³‡æ–™
    - Better convergence è¼ƒå¥½çš„æ”¶æ–‚æ€§

    Supported parameters æ”¯æ´åƒæ•¸ï¼š
        - epochs: Training epochs è¨“ç·´è¼ªæ•¸ (default é è¨­ 300)
        - batch_size: Batch size æ‰¹æ¬¡å¤§å° (default é è¨­ 500)
        - encoder_layers: Encoder layer configuration ç·¨ç¢¼å™¨å±¤è¨­å®š (default é è¨­ (128, 128))
        - decoder_layers: Decoder layer configuration è§£ç¢¼å™¨å±¤è¨­å®š (default é è¨­ (128, 128))
        - compress_dims: Compression dimensions å£“ç¸®ç¶­åº¦ (default é è¨­ (128, 128))
        - decompress_dims: Decompression dimensions è§£å£“ç¸®ç¶­åº¦ (default é è¨­ (128, 128))
        - embedding_dim: Embedding dimension åµŒå…¥ç¶­åº¦ (default é è¨­ 128)
        - l2scale: L2 regularization coefficient L2 æ­£å‰‡åŒ–ä¿‚æ•¸ (default é è¨­ 1e-5)
        - loss_factor: Loss factor æå¤±å› å­ (default é è¨­ 2)
        - enforce_min_max_values: Enforce numerical range constraints å¼·åˆ¶æ•¸å€¼ç¯„åœé™åˆ¶
        - enforce_rounding: Force rounding for integer columns å¼·åˆ¶æ•´æ•¸æ¬„ä½å››æ¨äº”å…¥
    """

    def _initialize_synthesizer(self):
        """Initialize TVAE synthesizer åˆå§‹åŒ– TVAE synthesizer"""
        from sdv.single_table import TVAESynthesizer

        init_params = {
            "metadata": self._sdv_metadata,
            "enforce_rounding": self.config.get("enforce_rounding", True),
            "enforce_min_max_values": self.config.get("enforce_min_max_values", True),
            "epochs": self.config.get("epochs", 300),
            "batch_size": self.config.get("batch_size", 500),
            "cuda": self.use_cuda,  # Enable/disable CUDA å•Ÿç”¨/ç¦ç”¨ CUDA
        }

        # Add network architecture parameters æ·»åŠ ç¶²çµ¡çµæ§‹åƒæ•¸
        if "encoder_layers" in self.config:
            # Convert list to tuple å°‡åˆ—è¡¨è½‰æ›ç‚º tuple
            init_params["compress_dims"] = tuple(self.config["encoder_layers"])

        if "decoder_layers" in self.config:
            init_params["decompress_dims"] = tuple(self.config["decoder_layers"])

        # Add other optional parameters æ·»åŠ å…¶ä»–å¯é¸åƒæ•¸
        optional_params = ["embedding_dim", "l2scale", "loss_factor", "verbose"]

        for param in optional_params:
            if param in self.config:
                init_params[param] = self.config[param]

        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter("always")
            self._synthesizer = TVAESynthesizer(**init_params)
            for warning in w:
                self.logger.debug(f"Warning during initialization: {warning.message}")


# Provide aliases for convenience ç‚ºäº†æ–¹ä¾¿ä½¿ç”¨ï¼Œæä¾›ä¸€å€‹åˆ¥åå°ç…§
SDV_METHODS = {
    "gaussiancopula": SDV_GaussianCopula,
    "ctgan": SDV_CTGAN,
    "copulagan": SDV_CopulaGAN,
    "tvae": SDV_TVAE,
}


if __name__ == "__main__":
    # Simple test program ç°¡å–®çš„æ¸¬è©¦ç¨‹å¼
    print("SDV Custom Methods for PETsARD")
    print("=" * 50)
    print("\nAvailable Methods å¯ç”¨çš„æ–¹æ³•ï¼š")
    for method_name, method_class in SDV_METHODS.items():
        print(f"  - {method_name}: {method_class.__name__}")
        print(f"    {method_class.__doc__.strip().split('suitable for')[0].strip()}")

    print("\n" + "=" * 50)
    print("Usage ä½¿ç”¨æ–¹å¼ï¼š")
    print("1. Set in YAML: method: custom_method åœ¨ YAML ä¸­è¨­å®š method: custom_method")
    print(
        "2. Specify: module_path: sdv-custom-methods.py æŒ‡å®š module_path: sdv-custom-methods.py"
    )
    print(
        "3. Specify: class_name: SDV_GaussianCopula (or other methods) æŒ‡å®š class_name: SDV_GaussianCopula (æˆ–å…¶ä»–æ–¹æ³•)"
    )
    print(
        "\nFor complete examples, see sdv-custom-methods.yaml è«‹åƒè€ƒ sdv-custom-methods.yaml ç²å–å®Œæ•´ç¯„ä¾‹"
    )
